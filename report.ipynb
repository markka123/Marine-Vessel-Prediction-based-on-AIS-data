{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Important to note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each group submits three things:\n",
    "- Select two predictions on Kaggle\n",
    "- Two short notebooks that contain everything needed to reproduce the two selected Kaggle predictions\n",
    "- A report (this document) summarizes all steps in our group work\n",
    "\n",
    "\n",
    "Need to remember:\n",
    "- Begin all notebooks with full names, student IDs (The one on the student card) and Kaggle team name\n",
    "- Project deadline is 22.00 at 10. November\n",
    "- The two short notebooks need to be able to reproduce the kaggle score!\n",
    "- Short notebooks need to use less than 12 hours to run\n",
    "- Use clear section titles in the report so that it is easy to find all parts under 1.3 Possible deductions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Need to remember"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Include blackboard group number in kaggle name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Possible deductions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Late delivery: -30 points\n",
    "- No exploratory data analysis: -3 points (Need to do four of: Search domain knowledge, Check if data is intuitive, Understand how the data was generated, Explore individual features, Clean up features)\n",
    "- Only one type of predictor used (does not apply to the short notebooks): -3 points\n",
    "- No feature engineering: -3 points\n",
    "- No model interpretation: -3 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Tips and Tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Choose the second submission model to be a more generalized version and (maybe) worse performing on the public kaggle leaderboard. They use the best of these two to calculate grades.\n",
    "- Notebooks can store temporary results (e.g after feature engineering) as disk-files\n",
    "- It is allowed to use constant hyperparameters etc. in the short notebooks as long as the report shows how we obtained them (e.g found by hyperparameter tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Code/model related tips and tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of all code/model-related tips and tricks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extract meaningful features from the additional datasets\n",
    "- Create a feature for whether the ship is moored or not\n",
    "- Must handle missing values suitably, can be holes in the data [interpolation]\n",
    "- Tune to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Search Domain Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Check if data is intuitive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Understand how the data was generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Explore individual features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Clean up features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predictors and Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 GRU model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.11 Version 1 - Submission 1 (701.553)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ais_train_data_path = '../../Project materials/ais_train.csv'\n",
    "ais_data_train = pd.read_csv(ais_train_data_path, sep='|')\n",
    "\n",
    "ship_train_groups = ais_data_train.groupby('vesselId')\n",
    "ship_train_dataframes = {ship_id: group for ship_id, group in ship_train_groups}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_timeseries = []\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "sequence_length = 5\n",
    "\n",
    "\n",
    "for ship_id, df in ship_train_dataframes.items():\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "    df['hour'] = df['time'].dt.hour\n",
    "    df['minute'] = df['time'].dt.minute\n",
    "    df['second'] = df['time'].dt.second\n",
    "\n",
    "    features = df[['hour', 'minute', 'second', 'longitude', 'latitude', 'sog', 'cog']].values\n",
    "\n",
    "    features_normalized = scaler.fit_transform(features)\n",
    "\n",
    "    for i in range(len(features_normalized) - sequence_length):\n",
    "        timeseries = features_normalized[i:i+sequence_length+1]\n",
    "        all_timeseries.append(timeseries)\n",
    "\n",
    "\n",
    "all_timeseries = np.array(all_timeseries)\n",
    "\n",
    "X_data = all_timeseries[:, :-1, :]\n",
    "Y_data = all_timeseries[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUnet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(GRUnet, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        hidden_initialize = torch.zeros(self.num_layers, X.size(0), self.hidden_size).to(X.device)\n",
    "\n",
    "        out, _ = self.gru(X, hidden_initialize)\n",
    "\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "#Model parameters:\n",
    "input_size = 7\n",
    "hidden_size = 64   #Random guess on what is best\n",
    "output_size = 7     \n",
    "num_layers = 2 \n",
    "\n",
    "GRU_model = GRUnet(input_size=input_size, hidden_size=hidden_size, output_size=output_size, num_layers=num_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(GRU_model.parameters(), lr = learning_rate)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "\n",
    "#Preprocess data:\n",
    "\n",
    "X_tensor = torch.tensor(X_data, dtype=torch.float32)\n",
    "Y_tensor = torch.tensor(Y_data, dtype=torch.float32)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_tensor, Y_tensor)\n",
    "data_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle = True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in data_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = GRU_model(inputs)\n",
    "\n",
    "        loss = loss_function(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(GRU_model.state_dict(), \"gru_model_test.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_GRU_model = GRUnet(input_size=input_size, hidden_size=hidden_size, output_size=output_size, num_layers=num_layers)\n",
    "loaded_GRU_model.load_state_dict(torch.load(\"gru_model_test.pth\"))\n",
    "\n",
    "loaded_GRU_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ais_test_data_path = '../../Project materials/ais_test.csv'\n",
    "ais_data_test = pd.read_csv(ais_test_data_path)\n",
    "unique_ship_ids = ais_data_test['vesselId'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ship_predictions = defaultdict(dict)\n",
    "\n",
    "for ship_id in unique_ship_ids:\n",
    "    if ship_id not in ship_train_dataframes:\n",
    "        print(f\"No training data available for ship_id: {ship_id}\")\n",
    "        continue\n",
    "\n",
    "    df = ship_train_dataframes[ship_id]\n",
    "    \n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    \n",
    "    df['hour'] = df['time'].dt.hour\n",
    "    df['minute'] = df['time'].dt.minute\n",
    "    df['second'] = df['time'].dt.second\n",
    "\n",
    "    features = df[['hour', 'minute', 'second', 'longitude', 'latitude', 'sog', 'cog']].values\n",
    "    if len(features) < sequence_length:\n",
    "        print(f\"Not enough historical data to predict for ship_id: {ship_id}\")\n",
    "        continue\n",
    "\n",
    "    input_sequence = features[-sequence_length:]\n",
    "    scaler = MinMaxScaler().fit(features)\n",
    "    input_sequence_normalized = scaler.transform(input_sequence)\n",
    "\n",
    "    input_tensor = torch.tensor(input_sequence_normalized, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    ship_test_times = pd.to_datetime(ais_data_test[ais_data_test['vesselId'] == ship_id]['time'])\n",
    "    farthest_time = ship_test_times.max()\n",
    "\n",
    "    last_known_time = df['time'].iloc[-1]\n",
    "    total_steps_needed = int((farthest_time - last_known_time).total_seconds() // (20 * 60))\n",
    "\n",
    "    current_input = input_tensor.clone()\n",
    "\n",
    "    for step in range(1, total_steps_needed + 1):\n",
    "        with torch.no_grad():\n",
    "            next_position = loaded_GRU_model(current_input)\n",
    "\n",
    "        prediction_time = last_known_time + pd.Timedelta(minutes=20 * step)\n",
    "        prediction_np = next_position.cpu().numpy()\n",
    "        prediction_original_scale = scaler.inverse_transform(prediction_np.reshape(1, -1))\n",
    "        \n",
    "        ship_predictions[ship_id][prediction_time] = prediction_original_scale[0]\n",
    "\n",
    "        current_input = torch.cat((current_input[:, 1:, :], next_position.unsqueeze(0)), dim=1)\n",
    "\n",
    "predictions_list = []\n",
    "\n",
    "for idx, row in ais_data_test.iterrows():\n",
    "    ship_id = row['vesselId']\n",
    "    target_time = pd.to_datetime(row['time']).round('min')\n",
    "\n",
    "    target_time_str = target_time.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "    if ship_id in ship_predictions:\n",
    "        if target_time_str in ship_predictions[ship_id]:\n",
    "            prediction = ship_predictions[ship_id][target_time_str]\n",
    "        else:\n",
    "\n",
    "            available_times = list(ship_predictions[ship_id].keys())\n",
    "            closest_time_str = min(available_times, key=lambda x: abs(pd.Timestamp(x) - target_time))\n",
    "            prediction = ship_predictions[ship_id][closest_time_str]\n",
    "\n",
    "        predictions_list.append({\n",
    "            'ship_id': ship_id,\n",
    "            'time': target_time,\n",
    "            'predicted_latitude': prediction[4],  \n",
    "            'predicted_longitude': prediction[3],\n",
    "            'predicted_sog': prediction[5],\n",
    "            'predicted_cog': prediction[6]\n",
    "        })\n",
    "\n",
    "predictions_df = pd.DataFrame(predictions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = predictions_df[['predicted_longitude', 'predicted_latitude']]\n",
    "\n",
    "# Write to CSV file with a header\n",
    "csv_file_path = 'submission.csv'\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "csv_data.to_csv(csv_file_path, index=True, index_label = 'ID', header=['longitude_predicted','latitude_predicted'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
