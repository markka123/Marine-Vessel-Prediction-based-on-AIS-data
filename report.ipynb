{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Important to note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each group submits three things:\n",
    "- Select two predictions on Kaggle\n",
    "- Two short notebooks that contain everything needed to reproduce the two selected Kaggle predictions\n",
    "- A report (this document) summarizes all steps in our group work\n",
    "\n",
    "\n",
    "Need to remember:\n",
    "- Begin all notebooks with full names, student IDs (The one on the student card) and Kaggle team name\n",
    "- Project deadline is 22.00 at 10. November\n",
    "- The two short notebooks need to be able to reproduce the kaggle score!\n",
    "- Short notebooks need to use less than 12 hours to run\n",
    "- Use clear section titles in the report so that it is easy to find all parts under 1.3 Possible deductions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Need to remember"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Include blackboard group number in kaggle name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Possible deductions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Late delivery: -30 points\n",
    "- No exploratory data analysis: -3 points (Need to do four of: Search domain knowledge, Check if data is intuitive, Understand how the data was generated, Explore individual features, Clean up features)\n",
    "- Only one type of predictor used (does not apply to the short notebooks): -3 points\n",
    "- No feature engineering: -3 points\n",
    "- No model interpretation: -3 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Tips and Tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Choose the second submission model to be a more generalized version and (maybe) worse performing on the public kaggle leaderboard. They use the best of these two to calculate grades.\n",
    "- Notebooks can store temporary results (e.g after feature engineering) as disk-files\n",
    "- It is allowed to use constant hyperparameters etc. in the short notebooks as long as the report shows how we obtained them (e.g found by hyperparameter tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Code/model related tips and tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of all code/model-related tips and tricks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extract meaningful features from the additional datasets\n",
    "- Create a feature for whether the ship is moored or not\n",
    "- Must handle missing values suitably, can be holes in the data [interpolation]\n",
    "- Tune to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Search Domain Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vessels ususally visits the same ports -> historic port visits can be used to infer most likely destination port\n",
    "\n",
    "TODO:\n",
    "- Research what all navstat values mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Check if data is intuitive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Understand how the data was generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Explore individual features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Clean up features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predictors and Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 GRU model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.11 Version 1 - Submission 1 (701.553)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ais_train_data_path = '../../Project materials/ais_train.csv'\n",
    "ais_data_train = pd.read_csv(ais_train_data_path, sep='|')\n",
    "\n",
    "ship_train_groups = ais_data_train.groupby('vesselId')\n",
    "ship_train_dataframes = {ship_id: group for ship_id, group in ship_train_groups}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_timeseries = []\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "sequence_length = 5\n",
    "\n",
    "\n",
    "for ship_id, df in ship_train_dataframes.items():\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "    df['hour'] = df['time'].dt.hour\n",
    "    df['minute'] = df['time'].dt.minute\n",
    "    df['second'] = df['time'].dt.second\n",
    "\n",
    "    features = df[['hour', 'minute', 'second', 'longitude', 'latitude', 'sog', 'cog']].values\n",
    "\n",
    "    features_normalized = scaler.fit_transform(features)\n",
    "\n",
    "    for i in range(len(features_normalized) - sequence_length):\n",
    "        timeseries = features_normalized[i:i+sequence_length+1]\n",
    "        all_timeseries.append(timeseries)\n",
    "\n",
    "\n",
    "all_timeseries = np.array(all_timeseries)\n",
    "\n",
    "X_data = all_timeseries[:, :-1, :]\n",
    "Y_data = all_timeseries[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUnet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(GRUnet, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        hidden_initialize = torch.zeros(self.num_layers, X.size(0), self.hidden_size).to(X.device)\n",
    "\n",
    "        out, _ = self.gru(X, hidden_initialize)\n",
    "\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "#Model parameters:\n",
    "input_size = 7\n",
    "hidden_size = 64   #Random guess on what is best\n",
    "output_size = 7     \n",
    "num_layers = 2 \n",
    "\n",
    "GRU_model = GRUnet(input_size=input_size, hidden_size=hidden_size, output_size=output_size, num_layers=num_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(GRU_model.parameters(), lr = learning_rate)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "\n",
    "#Preprocess data:\n",
    "\n",
    "X_tensor = torch.tensor(X_data, dtype=torch.float32)\n",
    "Y_tensor = torch.tensor(Y_data, dtype=torch.float32)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_tensor, Y_tensor)\n",
    "data_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle = True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in data_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = GRU_model(inputs)\n",
    "\n",
    "        loss = loss_function(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(GRU_model.state_dict(), \"gru_model_test.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_GRU_model = GRUnet(input_size=input_size, hidden_size=hidden_size, output_size=output_size, num_layers=num_layers)\n",
    "loaded_GRU_model.load_state_dict(torch.load(\"gru_model_test.pth\"))\n",
    "\n",
    "loaded_GRU_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ais_test_data_path = '../../Project materials/ais_test.csv'\n",
    "ais_data_test = pd.read_csv(ais_test_data_path)\n",
    "unique_ship_ids = ais_data_test['vesselId'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ship_predictions = defaultdict(dict)\n",
    "\n",
    "for ship_id in unique_ship_ids:\n",
    "    if ship_id not in ship_train_dataframes:\n",
    "        print(f\"No training data available for ship_id: {ship_id}\")\n",
    "        continue\n",
    "\n",
    "    df = ship_train_dataframes[ship_id]\n",
    "    \n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    \n",
    "    df['hour'] = df['time'].dt.hour\n",
    "    df['minute'] = df['time'].dt.minute\n",
    "    df['second'] = df['time'].dt.second\n",
    "\n",
    "    features = df[['hour', 'minute', 'second', 'longitude', 'latitude', 'sog', 'cog']].values\n",
    "    if len(features) < sequence_length:\n",
    "        print(f\"Not enough historical data to predict for ship_id: {ship_id}\")\n",
    "        continue\n",
    "\n",
    "    input_sequence = features[-sequence_length:]\n",
    "    scaler = MinMaxScaler().fit(features)\n",
    "    input_sequence_normalized = scaler.transform(input_sequence)\n",
    "\n",
    "    input_tensor = torch.tensor(input_sequence_normalized, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    ship_test_times = pd.to_datetime(ais_data_test[ais_data_test['vesselId'] == ship_id]['time'])\n",
    "    farthest_time = ship_test_times.max()\n",
    "\n",
    "    last_known_time = df['time'].iloc[-1]\n",
    "    total_steps_needed = int((farthest_time - last_known_time).total_seconds() // (20 * 60))\n",
    "\n",
    "    current_input = input_tensor.clone()\n",
    "\n",
    "    for step in range(1, total_steps_needed + 1):\n",
    "        with torch.no_grad():\n",
    "            next_position = loaded_GRU_model(current_input)\n",
    "\n",
    "        prediction_time = last_known_time + pd.Timedelta(minutes=20 * step)\n",
    "        prediction_np = next_position.cpu().numpy()\n",
    "        prediction_original_scale = scaler.inverse_transform(prediction_np.reshape(1, -1))\n",
    "        \n",
    "        ship_predictions[ship_id][prediction_time] = prediction_original_scale[0]\n",
    "\n",
    "        current_input = torch.cat((current_input[:, 1:, :], next_position.unsqueeze(0)), dim=1)\n",
    "\n",
    "predictions_list = []\n",
    "\n",
    "for idx, row in ais_data_test.iterrows():\n",
    "    ship_id = row['vesselId']\n",
    "    target_time = pd.to_datetime(row['time']).round('min')\n",
    "\n",
    "    target_time_str = target_time.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "    if ship_id in ship_predictions:\n",
    "        if target_time_str in ship_predictions[ship_id]:\n",
    "            prediction = ship_predictions[ship_id][target_time_str]\n",
    "        else:\n",
    "\n",
    "            available_times = list(ship_predictions[ship_id].keys())\n",
    "            closest_time_str = min(available_times, key=lambda x: abs(pd.Timestamp(x) - target_time))\n",
    "            prediction = ship_predictions[ship_id][closest_time_str]\n",
    "\n",
    "        predictions_list.append({\n",
    "            'ship_id': ship_id,\n",
    "            'time': target_time,\n",
    "            'predicted_latitude': prediction[4],  \n",
    "            'predicted_longitude': prediction[3],\n",
    "            'predicted_sog': prediction[5],\n",
    "            'predicted_cog': prediction[6]\n",
    "        })\n",
    "\n",
    "predictions_df = pd.DataFrame(predictions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = predictions_df[['predicted_longitude', 'predicted_latitude']]\n",
    "\n",
    "csv_file_path = 'submission.csv'\n",
    "\n",
    "csv_data.to_csv(csv_file_path, index=True, index_label = 'ID', header=['longitude_predicted','latitude_predicted'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.12 Version 2 - Submission 2 (749.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ais_train_data_path = '../../Project materials/ais_train.csv'\n",
    "#ports_data_path = '../../Project materials/ports.csv'\n",
    "vessels_data_path = '../../Project materials/vessels.csv'\n",
    "\n",
    "\n",
    "ais_data_train = pd.read_csv(ais_train_data_path, sep='|')\n",
    "#ports = pd.read_csv(ports_data_path, sep='|')\n",
    "vessels = pd.read_csv(vessels_data_path, sep='|')\n",
    "\n",
    "ship_train_ais_groups = ais_data_train.groupby('vesselId')\n",
    "ship_train_dataframes = {ship_id: group for ship_id, group in ship_train_ais_groups}\n",
    "\n",
    "\n",
    "vessels = vessels.set_index('vesselId')\n",
    "#ports = ports.set_index('portId')\n",
    "\n",
    "\n",
    "#Handle NAN values:\n",
    "columns_to_fill = ['GT', 'length', 'breadth', 'enginePower']\n",
    "\n",
    "for column in columns_to_fill:\n",
    "    median_value = vessels[column].median()  # Calculate the median value of the column\n",
    "    vessels[column] = vessels[column].fillna(median_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scalers\n",
    "time_series_scaler = MinMaxScaler()\n",
    "vessel_features_scaler = MinMaxScaler()\n",
    "\n",
    "sequence_length = 5\n",
    "\n",
    "# Collect all vessel features for scaling\n",
    "all_vessel_features = []\n",
    "all_features = []\n",
    "\n",
    "for ship_id, df in ship_train_dataframes.items():\n",
    "    if ship_id in vessels.index:\n",
    "        vessel_features = vessels.loc[ship_id][['length', 'breadth', 'enginePower', 'GT']].values\n",
    "        all_vessel_features.append(vessel_features)\n",
    "\n",
    "    # Collect time-series features for scaling\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    #df['etaRaw'] = df['etaRaw'].apply(lambda x: f\"2024-{x}\" if pd.notna(x) and isinstance(x, str) else np.nan)\n",
    "    #df['etaRaw'] = pd.to_datetime(df['etaRaw'], format='%Y-%m-%d %H:%M', errors='coerce')\n",
    "    df['hour'] = df['time'].dt.hour\n",
    "    df['minute'] = df['time'].dt.minute\n",
    "    df['second'] = df['time'].dt.second\n",
    "    #df['time_to_eta'] = (df['etaRaw'] - df['time']).dt.total_seconds() / 3600\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        ais_features = [\n",
    "            row['hour'], row['minute'], row['second'], \n",
    "            row['longitude'], row['latitude'], row['sog'], \n",
    "            row['cog'], row['rot'], row['heading'], row['navstat']\n",
    "        ]\n",
    "        all_features.append(ais_features)\n",
    "\n",
    "# Convert collected features to numpy arrays and fit the scalers\n",
    "all_vessel_features = np.array(all_vessel_features, dtype=np.float32)\n",
    "all_features = np.array(all_features, dtype=np.float32)\n",
    "\n",
    "vessel_features_scaler.fit(all_vessel_features)\n",
    "time_series_scaler.fit(all_features)\n",
    "\n",
    "# Transform each ship's data consistently using the fitted scalers\n",
    "all_timeseries = []\n",
    "vessel_features_list = []\n",
    "\n",
    "for ship_id, df in ship_train_dataframes.items():\n",
    "    if ship_id in vessels.index:\n",
    "        # Extract and scale vessel-specific features\n",
    "        vessel_features = vessels.loc[ship_id][['length', 'breadth', 'enginePower', 'GT']].values.reshape(1, -1)\n",
    "        vessel_features = vessel_features_scaler.transform(vessel_features).flatten()\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    # Extract and transform time-series features using the same fitted scaler\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    # df['etaRaw'] = df['etaRaw'].apply(lambda x: f\"2024-{x}\" if pd.notna(x) and isinstance(x, str) else np.nan)\n",
    "    # df['etaRaw'] = pd.to_datetime(df['etaRaw'], format='%Y-%m-%d %H:%M', errors='coerce')\n",
    "    df['hour'] = df['time'].dt.hour\n",
    "    df['minute'] = df['time'].dt.minute\n",
    "    df['second'] = df['time'].dt.second\n",
    "    #df['time_to_eta'] = (df['etaRaw'] - df['time']).dt.total_seconds() / 3600\n",
    "\n",
    "    features_list = []\n",
    "    for idx, row in df.iterrows():\n",
    "        ais_features = [\n",
    "            row['hour'], row['minute'], row['second'], \n",
    "            row['longitude'], row['latitude'], row['sog'], \n",
    "            row['cog'], row['rot'], row['heading'], row['navstat']\n",
    "        ]\n",
    "        features_list.append(ais_features)\n",
    "\n",
    "    # Convert to numpy array and transform using the scaler\n",
    "    features = np.array(features_list, dtype=np.float32)\n",
    "    features_normalized = time_series_scaler.transform(features)\n",
    "\n",
    "    # Create time-series data for training\n",
    "    for i in range(len(features_normalized) - sequence_length):\n",
    "        timeseries = features_normalized[i:i + sequence_length + 1]\n",
    "        all_timeseries.append(timeseries)\n",
    "\n",
    "        # Store vessel-specific features for each sequence\n",
    "        vessel_features_list.append(vessel_features)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "all_timeseries = np.array(all_timeseries, dtype=np.float32)\n",
    "vessel_features_list = np.array(vessel_features_list, dtype=np.float32)\n",
    "\n",
    "# Split time-series into X (input sequences) and Y (target values)\n",
    "X_data = all_timeseries[:, :-1, :]  # Shape: (num_samples, sequence_length, num_features)\n",
    "Y_data = all_timeseries[:, -1, :]  # Shape: (num_samples, num_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUnetExtended(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, non_timestep_features_size):\n",
    "        super(GRUnetExtended, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_size + non_timestep_features_size, output_size)\n",
    "\n",
    "    def forward(self, X, non_timestep_features):\n",
    "\n",
    "        hidden_initialize = torch.zeros(self.num_layers, X.size(0), self.hidden_size).to(X.device)\n",
    "\n",
    "        out, _ = self.gru(X, hidden_initialize)\n",
    "\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        combined = torch.cat((out, non_timestep_features), dim=1)\n",
    "\n",
    "        out = self.fc(combined)\n",
    "\n",
    "        out = torch.tanh(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#Model parameters:\n",
    "input_size = 10\n",
    "hidden_size = 64   #Random guess on what is best\n",
    "output_size = 10     \n",
    "num_layers = 2 \n",
    "non_timestep_features_size = 4\n",
    "\n",
    "extended_GRU_model = GRUnetExtended(input_size=input_size, hidden_size=hidden_size, output_size=output_size, num_layers=num_layers, non_timestep_features_size=non_timestep_features_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShipDataset(Dataset):\n",
    "    def __init__(self, X_data, vessel_features_list, Y_data):\n",
    "        self.X_data = torch.tensor(X_data, dtype=torch.float32)\n",
    "        self.vessel_features = torch.tensor(vessel_features_list, dtype=torch.float32)\n",
    "        self.Y_data = torch.tensor(Y_data, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'time_series': self.X_data[idx],\n",
    "            'vessel_features': self.vessel_features[idx],\n",
    "            'target': self.Y_data[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_timeseries = np.array(all_timeseries, dtype=np.float32)\n",
    "vessel_features_list = np.array(vessel_features_list, dtype=np.float32)\n",
    "Y_data = np.array(Y_data, dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "dataset = ShipDataset(X_data, vessel_features_list, Y_data)\n",
    "train_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(extended_GRU_model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    extended_GRU_model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        # Extract the features from the batch\n",
    "        time_series = batch['time_series']\n",
    "        vessel_features = batch['vessel_features']\n",
    "        target = batch['target']\n",
    "\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = extended_GRU_model(time_series, vessel_features)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "# Print the gradient norms for all parameters\n",
    "        for name, param in extended_GRU_model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                grad_norm = param.grad.norm()\n",
    "        \n",
    "                if grad_norm > 10:  # Threshold for detecting unusually large gradients\n",
    "                    print(f\"Warning: Unusually large gradient detected for {name}: {grad_norm}\")\n",
    "\n",
    "\n",
    "        #torch.nn.utils.clip_grad_norm_(extended_GRU_model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track the loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print the average loss for each epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ais_test_data_path = '../../Project materials/ais_test.csv'\n",
    "ais_data_test = pd.read_csv(ais_test_data_path)\n",
    "unique_ship_ids = ais_data_test['vesselId'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store predictions for each ship\n",
    "ship_predictions = defaultdict(dict)\n",
    "\n",
    "for ship_id in unique_ship_ids:\n",
    "    if ship_id not in ship_train_dataframes:\n",
    "        print(f\"No training data available for ship_id: {ship_id}\")\n",
    "        continue\n",
    "\n",
    "    df = ship_train_dataframes[ship_id]\n",
    "\n",
    "    # Convert time column to datetime\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "    # Extract time features\n",
    "    df['hour'] = df['time'].dt.hour\n",
    "    df['minute'] = df['time'].dt.minute\n",
    "    df['second'] = df['time'].dt.second\n",
    "\n",
    "    # Calculate `time_to_eta`\n",
    "    # df['etaRaw'] = df['etaRaw'].apply(lambda x: f\"2024-{x}\" if pd.notna(x) else x)\n",
    "    # df['etaRaw'] = pd.to_datetime(df['etaRaw'], format='%Y-%m-%d %H:%M', errors='coerce')\n",
    "    # df['time_to_eta'] = (df['etaRaw'] - df['time']).dt.total_seconds() / 3600  # Time to ETA in hours\n",
    "\n",
    "    # # Extract schedule-specific features (e.g., port latitude and longitude)\n",
    "    # df['port_lat'] = df['portId'].apply(lambda x: ports.loc[x]['latitude'] if pd.notna(x) and x in ports.index else np.nan)\n",
    "    # df['port_lon'] = df['portId'].apply(lambda x: ports.loc[x]['longitude'] if pd.notna(x) and x in ports.index else np.nan)\n",
    "\n",
    "    # Extract features used during training\n",
    "    features = df[['hour', 'minute', 'second', 'longitude', 'latitude', 'sog', 'cog', 'rot', 'heading', 'navstat']]\n",
    "\n",
    "    # Handle missing values with different strategies based on feature context using .loc[]\n",
    "    # features.loc[:, 'longitude'] = features['longitude'].fillna(features['longitude'].mean())\n",
    "    # features.loc[:, 'latitude'] = features['latitude'].fillna(features['latitude'].mean())\n",
    "    # features.loc[:, 'sog'] = features['sog'].fillna(features['sog'].median())\n",
    "    # features.loc[:, 'cog'] = features['cog'].fillna(features['cog'].median())\n",
    "    # features.loc[:, 'time_to_eta'] = features['time_to_eta'].fillna(features['time_to_eta'].median())\n",
    "    # features.loc[:, ['port_lat', 'port_lon']] = features[['port_lat', 'port_lon']].fillna(features[['port_lat', 'port_lon']].median())\n",
    "\n",
    "    # # Convert features to numpy array after handling NaN values\n",
    "    features = features.values\n",
    "\n",
    "    if len(features) < sequence_length:\n",
    "        print(f\"Not enough historical data to predict for ship_id: {ship_id}\")\n",
    "        continue\n",
    "\n",
    "    # Use the already fitted time-series scaler to transform features\n",
    "    input_sequence_normalized = time_series_scaler.transform(features[-sequence_length:])\n",
    "    input_tensor = torch.tensor(input_sequence_normalized, dtype=torch.float32).unsqueeze(0)  # Shape: (1, sequence_length, num_features)\n",
    "\n",
    "    # Extract and normalize vessel-specific features using the already fitted scaler\n",
    "    if ship_id in vessels.index:\n",
    "        vessel_features = vessels.loc[ship_id][['length', 'breadth', 'enginePower', 'GT']].values.reshape(1, -1)\n",
    "        vessel_features_normalized = vessel_features_scaler.transform(vessel_features).flatten()\n",
    "    else:\n",
    "        print(f\"No vessel-specific features available for ship_id: {ship_id}\")\n",
    "        continue\n",
    "\n",
    "    vessel_features_tensor = torch.tensor(vessel_features_normalized, dtype=torch.float32).unsqueeze(0)  # Shape: (1, vessel_feature_size)\n",
    "\n",
    "    # Get the farthest prediction time needed from test data\n",
    "    ship_test_times = pd.to_datetime(ais_data_test[ais_data_test['vesselId'] == ship_id]['time'])\n",
    "    farthest_time = ship_test_times.max()\n",
    "\n",
    "    # Last known time in the training data\n",
    "    last_known_time = df['time'].iloc[-1]\n",
    "    total_steps_needed = int((farthest_time - last_known_time).total_seconds() // (20 * 60))  # Time difference in steps of 20 minutes\n",
    "\n",
    "    # Make predictions recursively for the required number of steps\n",
    "    current_input = input_tensor.clone()\n",
    "\n",
    "    for step in range(1, total_steps_needed + 1):\n",
    "        with torch.no_grad():\n",
    "            # Make the prediction using the model\n",
    "            next_position = loaded_extended_GRU_model(current_input, vessel_features_tensor)\n",
    "\n",
    "        # Prediction time for the next step\n",
    "        prediction_time = last_known_time + pd.Timedelta(minutes=20 * step)\n",
    "\n",
    "        # Convert the prediction back to the original scale for storing\n",
    "        prediction_np = next_position.cpu().numpy()\n",
    "        prediction_original_scale = time_series_scaler.inverse_transform(prediction_np.reshape(1, -1))\n",
    "\n",
    "        # Store the prediction\n",
    "        ship_predictions[ship_id][prediction_time] = prediction_original_scale[0]\n",
    "\n",
    "        # Re-normalize the prediction to feed it back into the model\n",
    "        prediction_normalized = time_series_scaler.transform(prediction_original_scale)\n",
    "\n",
    "        # Update the input tensor by removing the oldest time step and appending the new normalized prediction\n",
    "        next_position_tensor = torch.tensor(prediction_normalized, dtype=torch.float32).unsqueeze(0)\n",
    "        current_input = torch.cat((current_input[:, 1:, :], next_position_tensor), dim=1)\n",
    "\n",
    "\n",
    "# Create a list to store the final predictions for the test data\n",
    "predictions_list = []\n",
    "\n",
    "for idx, row in ais_data_test.iterrows():\n",
    "    ship_id = row['vesselId']\n",
    "    target_time = pd.to_datetime(row['time']).round('min')\n",
    "\n",
    "    if ship_id in ship_predictions:\n",
    "        if target_time in ship_predictions[ship_id]:\n",
    "            prediction = ship_predictions[ship_id][target_time]\n",
    "        else:\n",
    "            # If the exact target time is not found, find the closest prediction time\n",
    "            available_times = list(ship_predictions[ship_id].keys())\n",
    "            closest_time = min(available_times, key=lambda x: abs(pd.Timestamp(x) - target_time))\n",
    "            prediction = ship_predictions[ship_id][closest_time]\n",
    "\n",
    "        predictions_list.append({\n",
    "            'ship_id': ship_id,\n",
    "            'time': target_time,\n",
    "            'predicted_latitude': prediction[4],  # Assuming latitude is at index 4\n",
    "            'predicted_longitude': prediction[3],  # Assuming longitude is at index 3\n",
    "            'predicted_sog': prediction[5],  # Assuming speed over ground (sog) is at index 5\n",
    "            'predicted_cog': prediction[6]  # Assuming course over ground (cog) is at index 6\n",
    "        })\n",
    "\n",
    "# Convert the predictions list to a DataFrame\n",
    "predictions_df = pd.DataFrame(predictions_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = predictions_df[['predicted_longitude', 'predicted_latitude']]\n",
    "\n",
    "# Write to CSV file with a header\n",
    "csv_file_path = 'submission2.csv'\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "csv_data.to_csv(csv_file_path, index=True, index_label = 'ID', header=['longitude_predicted','latitude_predicted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Catboost med randomsearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 Version 1 - Submission 2 (686)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ais_data_train = pd.read_csv('ais_train.csv', sep='|')\n",
    "\n",
    "ship_train_groups = ais_data_train.groupby('vesselId')\n",
    "ship_train_dataframes = {ship_id: group for ship_id, group in ship_train_groups}\n",
    "\n",
    "all_timeseries = []\n",
    "scaler = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "raw_targets = []\n",
    "\n",
    "sequence_length = 5\n",
    "\n",
    "for ship_id, df in ship_train_dataframes.items():\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "    df['hour'] = df['time'].dt.hour\n",
    "    df['minute'] = df['time'].dt.minute\n",
    "    df['second'] = df['time'].dt.second\n",
    "\n",
    "    features = df[['hour', 'minute', 'second', 'longitude', 'latitude', 'sog', 'cog',]].values\n",
    "\n",
    "    for i in range(sequence_length, len(features)):\n",
    "        raw_targets.append(features[i, [3, 4, 5, 6]])\n",
    "\n",
    "    features_normalized = scaler.fit_transform(features)\n",
    "\n",
    "    for i in range(len(features_normalized) - sequence_length):\n",
    "        timeseries = features_normalized[i:i+sequence_length+1]\n",
    "        all_timeseries.append(timeseries)\n",
    "\n",
    "all_timeseries = np.array(all_timeseries)\n",
    "raw_targets = np.array(raw_targets)\n",
    "\n",
    "X_data = all_timeseries[:, :-1, :].reshape(-1, sequence_length * 7)\n",
    "Y_data = raw_targets   # Output is the next time step's features\n",
    "\n",
    "scaler_y.fit(Y_data)\n",
    "Y_data_normalized = scaler_y.transform(Y_data)\n",
    "\n",
    "X_data = X_data.astype('float32')\n",
    "Y_data_normalized = Y_data_normalized.astype('float32')\n",
    "print(all_timeseries.shape)\n",
    "print(X_data.shape)\n",
    "print(Y_data_normalized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser CatBoost-modellen\n",
    "catboost_model = CatBoostRegressor(\n",
    "    loss_function='MultiRMSE',\n",
    "    verbose=0\n",
    ")\n",
    "# Definer hyperparameter-rutenett for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'depth': [4, 6],  # Flere verdier for depth\n",
    "    'learning_rate': [0.01, 0.05],  # Flere valgmuligheter for læringsrate\n",
    "    'iterations': [50, 100],  # Flere iterasjoner for mer trening\n",
    "    'l2_leaf_reg': [3, 5]\n",
    "}\n",
    "\n",
    "# Utfør RandomizedSearchCV for hyperparameter-tuning\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=catboost_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=5,  # Antall kombinasjoner å prøve\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=3,  # Cross-validation splits\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "# Tren RandomizedSearchCV på dataene\n",
    "random_search.fit(X_data, Y_data_normalized)\n",
    "\n",
    "# Beste modell fra RandomizedSearchCV\n",
    "best_catboost_model = random_search.best_estimator_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last inn testdata\n",
    "ais_data_test = pd.read_csv('ais_test.csv', sep=',')\n",
    "\n",
    "unique_ship_ids = ais_data_test['vesselId'].unique()\n",
    "\n",
    "ship_predictions = {}\n",
    "\n",
    "# Predict for each unique ship_id\n",
    "for ship_id in unique_ship_ids:\n",
    "\n",
    "    df = ship_train_dataframes[ship_id]\n",
    "    \n",
    "    # Ensure 'time' is in datetime format\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    \n",
    "    # Extract the last known sequence for this ship\n",
    "    df['hour'] = df['time'].dt.hour\n",
    "    df['minute'] = df['time'].dt.minute\n",
    "    df['second'] = df['time'].dt.second\n",
    "\n",
    "    features = df[['hour', 'minute', 'second', 'longitude', 'latitude', 'sog', 'cog']].values\n",
    "    if len(features) < sequence_length:\n",
    "        print(f\"Not enough historical data to predict for ship_id: {ship_id}\")\n",
    "        continue\n",
    "    \n",
    "\n",
    "    # Prepare the input sequence\n",
    "    input_sequence = features[-sequence_length:]\n",
    "    input_sequence_normalized = scaler.transform(input_sequence)\n",
    "    input_sequence_flattened = input_sequence_normalized.reshape(1, -1)\n",
    "\n",
    "    # Predict the next step using the best CatBoost model\n",
    "    prediction = best_catboost_model.predict(input_sequence_flattened)\n",
    "    prediction_original_scale = scaler_y.inverse_transform(prediction.reshape(1, -1))\n",
    "\n",
    "    # Store the prediction\n",
    "    ship_predictions[ship_id] = prediction_original_scale[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_list = []\n",
    "\n",
    "for idx, row in ais_data_test.iterrows():\n",
    "    ship_id = row['vesselId']\n",
    "\n",
    "    if ship_id in ship_predictions:\n",
    "        prediction = ship_predictions[ship_id]\n",
    "        predictions_list.append({\n",
    "            'ship_id': ship_id,\n",
    "            'time': row['time'],\n",
    "            'predicted_longitude': prediction[0],  # 'longitude' er første verdi\n",
    "            'predicted_latitude': prediction[1],   # 'latitude' er andre verdi\n",
    "            'predicted_sog': prediction[2],        # 'sog' er tredje verdi\n",
    "            'predicted_cog': prediction[3]         # 'cog' er fjerde verdi\n",
    "        })\n",
    "\n",
    "# Convert predictions to a DataFrame\n",
    "predictions_df = pd.DataFrame(predictions_list)\n",
    "\n",
    "print(predictions_df.head())\n",
    "\n",
    "# Write to CSV file with a header\n",
    "csv_data = predictions_df[['predicted_longitude', 'predicted_latitude']]\n",
    "\n",
    "csv_file_path = 'submission_catboost.csv'\n",
    "csv_data.to_csv(csv_file_path, index=True, index_label='ID', header=['longitude_predicted', 'latitude_predicted'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 LSTM (800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ais_data_train = pd.read_csv('ais_train.csv', sep='|')\n",
    "\n",
    "ship_train_groups = ais_data_train.groupby('vesselId')\n",
    "ship_train_dataframes = {ship_id: group for ship_id, group in ship_train_groups}\n",
    "\n",
    "all_timeseries = []\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "sequence_length = 5\n",
    "\n",
    "\n",
    "for ship_id, df in ship_train_dataframes.items():\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "    df['hour'] = df['time'].dt.hour\n",
    "    df['minute'] = df['time'].dt.minute\n",
    "    df['second'] = df['time'].dt.second\n",
    "\n",
    "    features = df[['hour', 'minute', 'second', 'longitude', 'latitude', 'sog', 'cog']].values\n",
    "\n",
    "    features_normalized = scaler.fit_transform(features)\n",
    "\n",
    "    for i in range(len(features_normalized) - sequence_length):\n",
    "        timeseries = features_normalized[i:i+sequence_length+1]\n",
    "        all_timeseries.append(timeseries)\n",
    "\n",
    "\n",
    "all_timeseries = np.array(all_timeseries)\n",
    "\n",
    "X_data = all_timeseries[:, :-1, :]\n",
    "Y_data = all_timeseries[:, -1, :]\n",
    "\n",
    "print(all_timeseries.shape)\n",
    "print(X_data.shape)\n",
    "print(Y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definer LSTM-modellen\n",
    "class LSTMnet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(LSTMnet, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Definer LSTM lagene\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Fullt koblet (fully connected) lag for utgang\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Initialiser skjult tilstand og cell state med nuller\n",
    "        hidden_initialize = torch.zeros(self.num_layers, X.size(0), self.hidden_size).to(X.device)\n",
    "        cell_initialize = torch.zeros(self.num_layers, X.size(0), self.hidden_size).to(X.device)\n",
    "\n",
    "        # Passer input gjennom LSTM\n",
    "        out, _ = self.lstm(X, (hidden_initialize, cell_initialize))\n",
    "\n",
    "        # Fullt koblet lag for å få utgang basert på siste tidssteg\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        return out\n",
    "\n",
    "# Modellparametere:\n",
    "input_size = 7      # Antall trekk i hver datapunkt\n",
    "hidden_size = 64    # Gjetning for passende størrelse\n",
    "output_size = 7     # Antall utganger\n",
    "num_layers = 2      # Antall lag i LSTM\n",
    "\n",
    "# Opprett LSTM-modellen\n",
    "LSTM_model = LSTMnet(input_size=input_size, hidden_size=hidden_size, output_size=output_size, num_layers=num_layers)\n",
    "\n",
    "print(LSTM_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Hyperparametere\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "# Optimaliseringsfunksjon og tapfunksjon\n",
    "optimizer = optim.Adam(LSTM_model.parameters(), lr=learning_rate)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# Simulert treningsdata (eksempel)\n",
    "# X_data og Y_data bør være forberedt på samme måte som tidligere nevnt\n",
    "X_data = np.random.rand(1000, 360, input_size)  # Dummy data: 1000 sekvenser med 360 tidssteg hver\n",
    "Y_data = np.random.rand(1000, output_size)      # Dummy labels\n",
    "\n",
    "# Konverter til PyTorch tensorer\n",
    "X_tensor = torch.tensor(X_data, dtype=torch.float32)\n",
    "Y_tensor = torch.tensor(Y_data, dtype=torch.float32)\n",
    "\n",
    "# Lag dataloaders\n",
    "train_dataset = TensorDataset(X_tensor, Y_tensor)\n",
    "data_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Treningsløkke\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in data_loader:\n",
    "\n",
    "        # Nullstill gradientene\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Fremoverpassering gjennom LSTM-modellen\n",
    "        outputs = LSTM_model(inputs)\n",
    "\n",
    "        # Beregn tap\n",
    "        loss = loss_function(outputs, targets)\n",
    "\n",
    "        # Tilbaketråkk (backpropagation)\n",
    "        loss.backward()\n",
    "\n",
    "        # Oppdater modellens parametere\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print tapet for hver epoke\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(LSTM_model.state_dict(), \"lstm_model_test.pth\")\n",
    "loaded_LSTM_model = LSTMnet(input_size=input_size, hidden_size=hidden_size, output_size=output_size, num_layers=num_layers)\n",
    "\n",
    "# Last inn lagrede parametre fra en fil\n",
    "loaded_LSTM_model.load_state_dict(torch.load(\"lstm_model_test.pth\"))\n",
    "\n",
    "# Sett modellen i evalueringsmodus\n",
    "loaded_LSTM_model.eval()\n",
    "\n",
    "print(\"LSTM-modellen er lastet inn og satt i evalueringsmodus.\")\n",
    "\n",
    "ais_data_test = pd.read_csv('ais_test.csv', sep=',')\n",
    "unique_ship_ids = ais_data_test['vesselId'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ship_predictions = defaultdict(dict)\n",
    "\n",
    "# Predict for each unique ship_id and store results\n",
    "for ship_id in unique_ship_ids:\n",
    "    if ship_id not in ship_train_dataframes:\n",
    "        print(f\"No training data available for ship_id: {ship_id}\")\n",
    "        continue\n",
    "\n",
    "    df = ship_train_dataframes[ship_id]\n",
    "    \n",
    "    # Ensure 'time' is in datetime format\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    \n",
    "    # Extract the last known sequence for this ship\n",
    "    df['hour'] = df['time'].dt.hour\n",
    "    df['minute'] = df['time'].dt.minute\n",
    "    df['second'] = df['time'].dt.second\n",
    "\n",
    "    features = df[['hour', 'minute', 'second', 'longitude', 'latitude', 'sog', 'cog']].values\n",
    "    if len(features) < sequence_length:\n",
    "        print(f\"Not enough historical data to predict for ship_id: {ship_id}\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the input sequence\n",
    "    input_sequence = features[-sequence_length:]\n",
    "    scaler = MinMaxScaler().fit(features)\n",
    "    input_sequence_normalized = scaler.transform(input_sequence)\n",
    "\n",
    "    # Convert to tensor and move to device\n",
    "    input_tensor = torch.tensor(input_sequence_normalized, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    # Determine the farthest time in the test data for this ship\n",
    "    ship_test_times = pd.to_datetime(ais_data_test[ais_data_test['vesselId'] == ship_id]['time'])\n",
    "    farthest_time = ship_test_times.max()\n",
    "\n",
    "    # Calculate how many steps are needed to reach the farthest time\n",
    "    last_known_time = df['time'].iloc[-1]\n",
    "    # Each step is 20 minutes, so we divide the total seconds by (20 * 60)\n",
    "    total_steps_needed = int((farthest_time - last_known_time).total_seconds() // (20 * 60))\n",
    "\n",
    "    # Predict all future steps up to the farthest time\n",
    "    current_input = input_tensor.clone()\n",
    "\n",
    "    for step in range(1, total_steps_needed + 1):\n",
    "        with torch.no_grad():\n",
    "            # Predict the next step\n",
    "            next_position = loaded_LSTM_model(current_input)\n",
    "\n",
    "        # Store the prediction with its corresponding timestamp\n",
    "        prediction_time = last_known_time + pd.Timedelta(minutes=20 * step)\n",
    "        prediction_np = next_position.cpu().numpy()\n",
    "        prediction_original_scale = scaler.inverse_transform(prediction_np.reshape(1, -1))\n",
    "        \n",
    "        # Add the prediction to the dictionary for the ship ID and timestamp\n",
    "        ship_predictions[ship_id][prediction_time] = prediction_original_scale[0]\n",
    "\n",
    "        # Update the input tensor by removing the oldest step and adding the predicted next step\n",
    "        current_input = torch.cat((current_input[:, 1:, :], next_position.unsqueeze(0)), dim=1)\n",
    "\n",
    "# Create a DataFrame to store the final results for each test point\n",
    "predictions_list = []\n",
    "\n",
    "# Iterate over the test data and extract the prediction from the stored dictionary\n",
    "for idx, row in ais_data_test.iterrows():\n",
    "    ship_id = row['vesselId']\n",
    "    target_time = pd.to_datetime(row['time']).round('min')\n",
    "\n",
    "    # Convert target time to string for lookup\n",
    "    target_time_str = target_time.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "    # Retrieve the stored prediction for this ship at the target time\n",
    "    if ship_id in ship_predictions:\n",
    "        if target_time_str in ship_predictions[ship_id]:\n",
    "            # Exact match found\n",
    "            prediction = ship_predictions[ship_id][target_time_str]\n",
    "        else:\n",
    "            # No exact match, find the closest timestamp\n",
    "            available_times = list(ship_predictions[ship_id].keys())\n",
    "            closest_time_str = min(available_times, key=lambda x: abs(pd.Timestamp(x) - target_time))\n",
    "            prediction = ship_predictions[ship_id][closest_time_str]\n",
    "\n",
    "        predictions_list.append({\n",
    "            'ship_id': ship_id,\n",
    "            'time': target_time,\n",
    "            'predicted_latitude': prediction[4],  # assuming columns are ['hour', 'minute', 'second', 'longitude', 'latitude', 'sog', 'cog']\n",
    "            'predicted_longitude': prediction[3],\n",
    "            'predicted_sog': prediction[5],\n",
    "            'predicted_cog': prediction[6]\n",
    "        })\n",
    "\n",
    "# Convert predictions to a DataFrame\n",
    "predictions_df = pd.DataFrame(predictions_list)\n",
    "\n",
    "print(predictions_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = predictions_df[['predicted_longitude', 'predicted_latitude']]\n",
    "\n",
    "# Write to CSV file with a header\n",
    "csv_file_path = 'submission2.csv'\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "csv_data.to_csv(csv_file_path, index=True, index_label = 'ID', header=['longitude_predicted','latitude_predicted'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
