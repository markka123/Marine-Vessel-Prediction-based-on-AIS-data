{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Framework for testing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from geopy.distance import geodesic\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ais_train_data_path = '../../Project materials/ais_train.csv'\n",
    "ais_test_data_path = '../../Project materials/ais_test.csv'\n",
    "ports_data_path = '../../Project materials/ports.csv'\n",
    "vessels_data_path = '../../Project materials/vessels.csv'\n",
    "schedules_data_path = '../../Project materials/schedules_to_may_2024.csv'\n",
    "\n",
    "\n",
    "\n",
    "ais_data_train = pd.read_csv(ais_train_data_path, sep='|')\n",
    "ais_data_test = pd.read_csv(ais_test_data_path, sep=',')\n",
    "ports = pd.read_csv(ports_data_path, sep='|')\n",
    "vessels = pd.read_csv(vessels_data_path, sep='|')\n",
    "schedules = pd.read_csv(schedules_data_path, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILTER OUT ALL SHIPS WITH LESS THAN 300 ENTRIES FROM THE TRAINING DATAFRAME:\n",
    "\n",
    "timesteps_per_ship = ais_data_train.groupby('vesselId').size()\n",
    "vessels_with_enough_timesteps = timesteps_per_ship[timesteps_per_ship >= 300].index\n",
    "\n",
    "# Step 3: Filter the main dataframe to keep only ships with 300 or more timesteps\n",
    "ais_data_train = ais_data_train[ais_data_train['vesselId'].isin(vessels_with_enough_timesteps)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or extract features that are to be tested:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Movement status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACT MOVEMENT STATUS FROM NAVSTAT:\n",
    "\n",
    "def categorize_navstat_contrast(navstat):\n",
    "    if navstat in [0, 8]:\n",
    "        return 1  # Underway\n",
    "    elif navstat in [2, 3, 4]:\n",
    "        return 0.5  # Restricted Movement\n",
    "    elif navstat in [1, 5, 6]:\n",
    "        return -1  # Stationary\n",
    "    else:\n",
    "        return 0  # Unknown\n",
    "\n",
    "ais_data_train['movement_status'] = ais_data_train['navstat'].apply(categorize_navstat_contrast)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Estimated time to destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess ETA RAW and time\n",
    "\n",
    "current_year = 2024\n",
    "ais_data_train['ETARAW_transformed'] = ais_data_train['etaRaw'].apply(lambda x: f\"{current_year}-{x}\")\n",
    "ais_data_train['ETARAW_transformed'] = pd.to_datetime(ais_data_train['ETARAW_transformed'], format='%Y-%m-%d %H:%M', errors='coerce')\n",
    "ais_data_train['time'] = pd.to_datetime(ais_data_train['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use linear interpolation to edit nan values\n",
    "\n",
    "ais_data_train =ais_data_train.sort_values(by=['vesselId', 'time']).reset_index(drop=True)\n",
    "grouped =ais_data_train.groupby('vesselId')\n",
    "ais_data_train['ETARAW_transformed'] = grouped['ETARAW_transformed'].apply(lambda x: x.interpolate(method='linear')).reset_index(level=0, drop=True)\n",
    "\n",
    "ais_data_train['estimated_time_to_destination'] = (ais_data_train['ETARAW_transformed'] -ais_data_train['time']).dt.total_seconds() / 3600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Time difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ais_data_train['time_difference'] = ais_data_train.groupby('vesselId')['time'].diff().dt.total_seconds() / 60\n",
    "ais_data_train['time_difference'] = ais_data_train['time_difference'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into testing and training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_ship_dataset(ship_dataset):\n",
    "\n",
    "    last_timestep = ship_dataset['time'].max()\n",
    "    five_days_ago = last_timestep - pd.Timedelta(days=5)\n",
    "\n",
    "    test_data = ship_dataset[ship_dataset['time'] > five_days_ago]\n",
    "    train_data = ship_dataset[ship_dataset['time'] <= five_days_ago]\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ais_data_train = ais_data_train.sort_values(by=['vesselId', 'time'])\n",
    "\n",
    "train_list = []\n",
    "test_list = []\n",
    "\n",
    "for vessel_id, group in ais_data_train.groupby('vesselId'):\n",
    "    train_data, test_data = split_ship_dataset(group)\n",
    "    train_list.append(train_data)\n",
    "    test_list.append(test_data)\n",
    "\n",
    "train_df = pd.concat(train_list)\n",
    "test_df = pd.concat(test_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1435417, 15)\n",
      "(85622, 15)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the one model per ship approach (Random Forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try:\n",
    "- Implement a random forest classifier for the movement status feature (Transform features between the numeric values to the labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_features = ['longitude', 'latitude', 'time_difference', ]\n",
    "new_features = ['longitude', 'latitude', 'time_difference', 'cog', 'sog', 'rot']\n",
    "\n",
    "unique_vessel_ids = ais_data_test['vesselId'].unique()\n",
    "\n",
    "\n",
    "#Hyperparameters\n",
    "sequence_length = 5\n",
    "number_of_estimatiors = 50\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_geodesic_distances = []\n",
    "new_geodesic_distances = []\n",
    "\n",
    "movement_status_mismatch_count = 0  \n",
    "total_movement_status_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Done with a ship\n",
      "Baseline Model - Average Geodesic Distance (km): 958.978766204436\n",
      "New Model - Average Geodesic Distance (km): 1013.0452410390714\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for vessel_id in unique_vessel_ids:\n",
    "\n",
    "    # EXTRACT DATA FOR THE VESSEL AT HAND:\n",
    "    vessel_df_train = train_df[train_df['vesselId'] == vessel_id]\n",
    "    vessel_df_test = test_df[test_df['vesselId'] == vessel_id]\n",
    "\n",
    "    vessel_df_train = vessel_df_train.sort_values(by='time')\n",
    "    vessel_df_test = vessel_df_test.sort_values(by='time')\n",
    "    vessel_df_test = vessel_df_test.reset_index(drop=True)\n",
    "\n",
    "    if len(vessel_df_train) < sequence_length:\n",
    "        print(f'Not enough historical data to predict for vessel_id: {vessel_id}')\n",
    "        continue\n",
    "\n",
    "    total_baseline_features = vessel_df_train[baseline_features].values\n",
    "    total_new_features = vessel_df_train[new_features].values  \n",
    "\n",
    "    # CREATE TIMESERIES:\n",
    "    X_baseline, y_baseline, X_new, y_new, X_mapped_new = [], [], [], [], []\n",
    "    for i in range(len(total_baseline_features) - sequence_length):\n",
    "        X_baseline.append(total_baseline_features[i:i + sequence_length])\n",
    "        X_new.append(total_new_features[i:i + sequence_length])\n",
    "\n",
    "        y_baseline.append(total_baseline_features[i + sequence_length])\n",
    "        y_new.append(total_new_features[i + sequence_length])\n",
    "\n",
    "    X_baseline = np.array(X_baseline)\n",
    "    X_new = np.array(X_new)\n",
    "    X_mapped_new = np.array(X_mapped_new)  \n",
    "    y_baseline = np.array(y_baseline)\n",
    "    y_new = np.array(y_new)\n",
    "\n",
    "    # DEFINE MODELS:\n",
    "    lat_baseline_model = RandomForestRegressor(n_estimators=number_of_estimatiors, random_state=random_state)\n",
    "    lon_baseline_model = RandomForestRegressor(n_estimators=number_of_estimatiors, random_state=random_state)\n",
    "    lat_new_model = RandomForestRegressor(n_estimators=number_of_estimatiors, random_state=random_state)\n",
    "    lon_new_model = RandomForestRegressor(n_estimators=number_of_estimatiors, random_state=random_state)\n",
    "    cog_new_model = RandomForestRegressor(n_estimators=number_of_estimatiors, random_state=random_state)\n",
    "    sog_new_model = RandomForestRegressor(n_estimators=number_of_estimatiors, random_state=random_state)\n",
    "    rot_new_model = RandomForestRegressor(n_estimators=number_of_estimatiors, random_state=random_state)\n",
    "\n",
    "    #mov_stat_classifier = RandomForestClassifier(n_estimators=number_of_estimatiors, random_state=random_state)\n",
    "\n",
    "    # TRAIN MODELS:\n",
    "    lat_baseline_model.fit(X_baseline.reshape(X_baseline.shape[0], -1), y_baseline[:, 1])\n",
    "    lon_baseline_model.fit(X_baseline.reshape(X_baseline.shape[0], -1), y_baseline[:, 0])\n",
    "    lat_new_model.fit(X_new.reshape(X_new.shape[0], -1), y_new[:, 1])\n",
    "    lon_new_model.fit(X_new.reshape(X_new.shape[0], -1), y_new[:, 0])\n",
    "    cog_new_model.fit(X_new.reshape(X_new.shape[0], -1), y_new[:, 3])\n",
    "    sog_new_model.fit(X_new.reshape(X_new.shape[0], -1), y_new[:, 4])\n",
    "    rot_new_model.fit(X_new.reshape(X_new.shape[0], -1), y_new[:, 5])\n",
    "\n",
    "    #y_mov_stat_class = np.vectorize(movement_status_mapping.get)(y_new[:, 3])\n",
    "    #mov_stat_classifier.fit(X_mapped_new.reshape(X_mapped_new.shape[0], -1), y_mov_stat_class)\n",
    "\n",
    "    # PREPARE FOR EVALUATION OF MODELS:\n",
    "    last_known_sequence_baseline = total_baseline_features[-sequence_length:]\n",
    "    last_known_sequence_new = total_new_features[-sequence_length:] \n",
    "\n",
    "    current_input_baseline = last_known_sequence_baseline.reshape(1, -1)\n",
    "    current_input_new = last_known_sequence_new.reshape(1, -1)\n",
    "\n",
    "    true_latitudes = vessel_df_test[baseline_features].iloc[:, 1]\n",
    "    true_longitudes = vessel_df_test[baseline_features].iloc[:, 0]\n",
    "\n",
    "    for step in range(len(vessel_df_test)):\n",
    "\n",
    "        # PREDICTIONS:\n",
    "        predicted_lat_baseline = lat_baseline_model.predict(current_input_baseline)[0]\n",
    "        predicted_lon_baseline = lon_baseline_model.predict(current_input_baseline)[0]\n",
    "        predicted_lat_new = lat_new_model.predict(current_input_new)[0]\n",
    "        predicted_lon_new = lon_new_model.predict(current_input_new)[0]\n",
    "        predicted_cog_new = cog_new_model.predict(current_input_new)[0]\n",
    "        predicted_sog_new = sog_new_model.predict(current_input_new)[0]\n",
    "        predicted_rot_new = rot_new_model.predict(current_input_new)[0]\n",
    "\n",
    "        #predicted_mov_stat_category = mov_stat_classifier.predict(current_input_mapped_new.reshape(1, -1))[0]\n",
    "\n",
    "        #predicted_mov_stat_new = inverse_movement_status_mapping[predicted_mov_stat_category]\n",
    "\n",
    "        # EVALUATION:\n",
    "        true_lat = true_latitudes[step]\n",
    "        true_lon = true_longitudes[step]\n",
    "\n",
    "        actual_coords = (true_lat, true_lon)\n",
    "        baseline_predicted_coords = (predicted_lat_baseline, predicted_lon_baseline)\n",
    "\n",
    "        geodesic_dist_baseline = geodesic(actual_coords, baseline_predicted_coords).kilometers\n",
    "        baseline_geodesic_distances.append(geodesic_dist_baseline)\n",
    "\n",
    "        new_predicted_coords = (predicted_lat_new, predicted_lon_new)\n",
    "        geodesic_dist_new = geodesic(actual_coords, new_predicted_coords).kilometers\n",
    "        new_geodesic_distances.append(geodesic_dist_new)\n",
    "\n",
    "\n",
    "        # FIND NEXT INPUTS:\n",
    "        next_input_baseline = np.array([predicted_lon_baseline, predicted_lat_baseline, 20])\n",
    "        current_input_baseline = np.hstack([current_input_baseline[:, 3:], next_input_baseline.reshape(1, -1)])\n",
    "\n",
    "        next_input_new = np.array([predicted_lon_new, predicted_lat_new, 20, predicted_cog_new, predicted_sog_new, predicted_rot_new])  \n",
    "        \n",
    "        current_input_new = np.hstack([current_input_new[:, 6:], next_input_new.reshape(1, -1)])\n",
    "        \n",
    "    print(\"Done with a ship\")\n",
    "\n",
    "average_geodesic_baseline = np.mean(baseline_geodesic_distances)\n",
    "average_geodesic_new = np.mean(new_geodesic_distances)\n",
    "\n",
    "# VISUALIZE RESULTS\n",
    "print(f\"Baseline Model - Average Geodesic Distance (km): {average_geodesic_baseline}\")\n",
    "print(f\"New Model - Average Geodesic Distance (km): {average_geodesic_new}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
