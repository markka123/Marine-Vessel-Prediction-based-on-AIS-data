{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Thoughts and ideas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.1 General"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One possible strategy:\n",
    "- Treat the prediction of future AIS data as a prediction task itself (X: Historic AIS and positions, Y: AIS data in next timestep) and create a model for this\n",
    "- Use the predicted AIS data as well as historic AIS data and positions to predict new position. \n",
    "\n",
    "\n",
    "Another:\n",
    "- Let a model use the previous timesteps to predict all information about the next timestep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.2 About the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.3 Research\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Article evaluating several models to predict ship trajectories\n",
    "\n",
    "Definitions:\n",
    "- Ship trajectory is the sequence if timestamped points Pi = {Ti, LATi, LONi, SOGi, COGi}\n",
    "\n",
    "\n",
    "Methodology:\n",
    "- Information from the first four timestamps are used to predict the next.\n",
    "- Implemented using a Pytorch framework\n",
    "- Use ADAM as optimizer\n",
    "- Use the following hyperparameters: Learning rate: 0.0001, epoch: 100, dropout: 0.5, Hidden size:128 (15), input/output dimensions: 2 and hidden layer: 1\n",
    "\n",
    "\n",
    "Interesting points\n",
    "- \"Deep learning exhibits remarkable performance in AIS data-driven ship trajectory prediction\"\n",
    "- \"Deep learning are in general better than machine learning for this application\"\n",
    "- Transformer, BI-GRU and GRU performs the best, transformer only outperforms on medium sized datasets\n",
    "- SVR is the best machine learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Brainstorming - 18.09.2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AIS - data:\n",
    "- Parameters that intuitively give us the next position (COG, SOG and ROT), (current position, ETARAW and PortID)\n",
    "- Should try merging navstat codes used to describe the same activity\n",
    "\n",
    "\n",
    "\n",
    "General:\n",
    "- Should somehow allow the algorithm to keep the last values - research different strategies (CNN or LSTM?)\n",
    "- Might want to use a classifier to predict features\n",
    "- Ship-ID is probably a pointless input for the classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Other\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gustav & co brukte autogluon: https://auto.gluon.ai/stable/index.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Load data into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>cog</th>\n",
       "      <th>sog</th>\n",
       "      <th>rot</th>\n",
       "      <th>heading</th>\n",
       "      <th>navstat</th>\n",
       "      <th>etaRaw</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>vesselId</th>\n",
       "      <th>portId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01 00:00:25</td>\n",
       "      <td>284.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>01-09 23:00</td>\n",
       "      <td>-34.74370</td>\n",
       "      <td>-57.85130</td>\n",
       "      <td>61e9f3a8b937134a3c4bfdf7</td>\n",
       "      <td>61d371c43aeaecc07011a37f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-01 00:00:36</td>\n",
       "      <td>109.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6</td>\n",
       "      <td>347</td>\n",
       "      <td>1</td>\n",
       "      <td>12-29 20:00</td>\n",
       "      <td>8.89440</td>\n",
       "      <td>-79.47939</td>\n",
       "      <td>61e9f3d4b937134a3c4bff1f</td>\n",
       "      <td>634c4de270937fc01c3a7689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-01 00:01:45</td>\n",
       "      <td>111.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>01-02 09:00</td>\n",
       "      <td>39.19065</td>\n",
       "      <td>-76.47567</td>\n",
       "      <td>61e9f436b937134a3c4c0131</td>\n",
       "      <td>61d3847bb7b7526e1adf3d19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-01 00:03:11</td>\n",
       "      <td>96.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "      <td>1</td>\n",
       "      <td>12-31 20:00</td>\n",
       "      <td>-34.41189</td>\n",
       "      <td>151.02067</td>\n",
       "      <td>61e9f3b4b937134a3c4bfe77</td>\n",
       "      <td>61d36f770a1807568ff9a126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-01 00:03:51</td>\n",
       "      <td>214.0</td>\n",
       "      <td>19.7</td>\n",
       "      <td>0</td>\n",
       "      <td>215</td>\n",
       "      <td>0</td>\n",
       "      <td>01-25 12:00</td>\n",
       "      <td>35.88379</td>\n",
       "      <td>-5.91636</td>\n",
       "      <td>61e9f41bb937134a3c4c0087</td>\n",
       "      <td>634c4de270937fc01c3a74f3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time    cog   sog  rot  heading  navstat       etaRaw  \\\n",
       "0  2024-01-01 00:00:25  284.0   0.7    0       88        0  01-09 23:00   \n",
       "1  2024-01-01 00:00:36  109.6   0.0   -6      347        1  12-29 20:00   \n",
       "2  2024-01-01 00:01:45  111.0  11.0    0      112        0  01-02 09:00   \n",
       "3  2024-01-01 00:03:11   96.4   0.0    0      142        1  12-31 20:00   \n",
       "4  2024-01-01 00:03:51  214.0  19.7    0      215        0  01-25 12:00   \n",
       "\n",
       "   latitude  longitude                  vesselId                    portId  \n",
       "0 -34.74370  -57.85130  61e9f3a8b937134a3c4bfdf7  61d371c43aeaecc07011a37f  \n",
       "1   8.89440  -79.47939  61e9f3d4b937134a3c4bff1f  634c4de270937fc01c3a7689  \n",
       "2  39.19065  -76.47567  61e9f436b937134a3c4c0131  61d3847bb7b7526e1adf3d19  \n",
       "3 -34.41189  151.02067  61e9f3b4b937134a3c4bfe77  61d36f770a1807568ff9a126  \n",
       "4  35.88379   -5.91636  61e9f41bb937134a3c4c0087  634c4de270937fc01c3a74f3  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "ais_train_data_path = '../../Project materials/ais_train.csv'\n",
    "\n",
    "\n",
    "ais_data_train = pd.read_csv(ais_train_data_path, sep='|')\n",
    "\n",
    "\n",
    "ais_data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframes for each ship-id:\n",
    "\n",
    "ship_train_groups = ais_data_train.groupby('vesselId')\n",
    "ship_train_dataframes = {ship_id: group for ship_id, group in ship_train_groups}\n",
    "\n",
    "#Split data into input and output. Input can now be accessed as ship_dataframes[shipID][0] and output as ship_dataframes[shipID][1]\n",
    "\n",
    "# for key in ship_train_dataframes:\n",
    "#     ship_train_dataframes[key] = [ship_train_dataframes[key].drop(columns=['latitude', 'longitude']), ship_train_dataframes[key][['latitude', 'longitude']]]\n",
    "\n",
    "\n",
    "# print(ship_train_dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Split data into X and Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Try to create predictions using simple models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 XG-boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_simple = xgb.XGBClassifier()\n",
    "\n",
    "\n",
    "# for key in ship_train_dataframes:\n",
    "#     xgb_simple.fit(ship_train_dataframes[key][0], ship_train_dataframes[key][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Attempting to implement a similar approach as in the article: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result: 701\n",
    "\n",
    "Weaknesses\n",
    "- Only uses Cog, Sog and previous position to predict\n",
    "- Hyperparameters are chosen on slump, not tuned (learning rate)\n",
    "- Model Arcitecture (number of layers and hidden sizes, number of previous timesteps that are used as input)\n",
    "- No regularizaion techniques (Dropout, etc.)\n",
    "- No iterpolation to fill empty values\n",
    "\n",
    "\n",
    "Next steps\n",
    "- Figure out how to add input features that are not timestep dependant\n",
    "- Create new features (moored, etc.)\n",
    "- Figure out how to handle holes in the data (interpolation)\n",
    "- Research how to tune hyperparameters\n",
    "- Add features to the timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Preprocess data into timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1518629, 6, 5)\n",
      "(1518629, 5, 5)\n",
      "(1518629, 5)\n"
     ]
    }
   ],
   "source": [
    "def categorize_navstat_contrast(navstat):\n",
    "    if navstat in [0, 8]:\n",
    "        return 1  # Underway\n",
    "    elif navstat in [2, 3, 4]:\n",
    "        return 0.5  # Restricted Movement\n",
    "    elif navstat in [1, 5, 6]:\n",
    "        return -1  # Stationary\n",
    "    else:\n",
    "        return 0  # Unknown\n",
    "\n",
    "all_timeseries = []\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "sequence_length = 5\n",
    "\n",
    "\n",
    "for ship_id, df in ship_train_dataframes.items():\n",
    "\n",
    "    df['movement_status'] = df['navstat'].apply(categorize_navstat_contrast)\n",
    "    features = df[['longitude', 'latitude', 'sog', 'cog', 'movement_status']].values\n",
    "\n",
    "    features_normalized = scaler.fit_transform(features)\n",
    "\n",
    "    for i in range(len(features_normalized) - sequence_length):\n",
    "        timeseries = features_normalized[i:i+sequence_length+1]\n",
    "        all_timeseries.append(timeseries)\n",
    "\n",
    "\n",
    "all_timeseries = np.array(all_timeseries)\n",
    "\n",
    "X_data = all_timeseries[:, :-1, :]\n",
    "Y_data = all_timeseries[:, -1, :]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(all_timeseries.shape)\n",
    "print(X_data.shape)\n",
    "print(Y_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 GRU - model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUnet(\n",
      "  (gru): GRU(5, 64, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=64, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class GRUnet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(GRUnet, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        hidden_initialize = torch.zeros(self.num_layers, X.size(0), self.hidden_size).to(X.device)\n",
    "\n",
    "        out, _ = self.gru(X, hidden_initialize)\n",
    "\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        out = torch.tanh(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "#Model parameters:\n",
    "input_size = 5\n",
    "hidden_size = 64   #Random guess on what is best\n",
    "output_size = 5     \n",
    "num_layers = 2 \n",
    "\n",
    "GRU_model = GRUnet(input_size=input_size, hidden_size=hidden_size, output_size=output_size, num_layers=num_layers)\n",
    "\n",
    "print(GRU_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0138\n",
      "Epoch [2/10], Loss: 0.0134\n",
      "Epoch [3/10], Loss: 0.0090\n",
      "Epoch [4/10], Loss: 0.0147\n",
      "Epoch [5/10], Loss: 0.0196\n",
      "Epoch [6/10], Loss: 0.0082\n",
      "Epoch [7/10], Loss: 0.0193\n",
      "Epoch [8/10], Loss: 0.0130\n",
      "Epoch [9/10], Loss: 0.0217\n",
      "Epoch [10/10], Loss: 0.0186\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters \n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(GRU_model.parameters(), lr = learning_rate)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "\n",
    "#Preprocess data:\n",
    "\n",
    "X_tensor = torch.tensor(X_data, dtype=torch.float32)\n",
    "Y_tensor = torch.tensor(Y_data, dtype=torch.float32)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_tensor, Y_tensor)\n",
    "data_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle = True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in data_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = GRU_model(inputs)\n",
    "\n",
    "        loss = loss_function(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Save model\n",
    "\n",
    "torch.save(GRU_model.state_dict(), \"gru_model_test.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRUnet(\n",
       "  (gru): GRU(5, 64, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=64, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_GRU_model = GRUnet(input_size=input_size, hidden_size=hidden_size, output_size=output_size, num_layers=num_layers)\n",
    "loaded_GRU_model.load_state_dict(torch.load(\"gru_model_test.pth\"))\n",
    "\n",
    "loaded_GRU_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def predict_ship_position(ship_id, time, model, sequence_length = 5):\n",
    "\n",
    "#     if ship_id not in ship_train_dataframes:\n",
    "#         print(f\"No training data available for ship_id: {ship_id}\")\n",
    "#         return None\n",
    "    \n",
    "#     ship_df = ship_train_dataframes[ship_id]\n",
    "\n",
    "#     ship_df['time'] = pd.to_datetime(ship_df['time'])\n",
    "\n",
    "#     ship_df = ship_df.sort_values(by='time').reset_index(drop=True)\n",
    "\n",
    "#     prediction_time = pd.to_datetime(time)\n",
    "\n",
    "#     last_known_time = df['time'].iloc[-1]\n",
    "#     time_diff = (prediction_time - last_known_time).total_seconds()\n",
    "    \n",
    "#     if time_diff <= 0:\n",
    "#         print(f\"Prediction time {prediction_time} is before or equal to the last known time {last_known_time}\")\n",
    "#         return None\n",
    "    \n",
    "#     steps_needed = int(time_diff//20)\n",
    "\n",
    "#     df['hour'] = df['time'].dt.hour\n",
    "#     df['minute'] = df['time'].dt.minute\n",
    "#     df['second'] = df['time'].dt.second\n",
    "\n",
    "#     features = df[['hour', 'minute', 'second', 'longitude', 'latitude', 'sog', 'cog']].values\n",
    "\n",
    "#     if len(features) < sequence_length:\n",
    "#         print(f\"Not enough historical data to predict for ship_id: {ship_id}\")\n",
    "#         return None\n",
    "\n",
    "#     input_sequence = features[-sequence_length:]\n",
    "\n",
    "#     input_sequence_normalized =scaler.transform(input_sequence)\n",
    "#     input_tensor = torch.tensor(input_sequence_normalized, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "#     for _ in range(steps_needed):\n",
    "#         with torch.no_grad():\n",
    "           \n",
    "#             next_position = model(input_tensor)\n",
    "\n",
    "        \n",
    "#         next_position_tensor = next_position.unsqueeze(0)\n",
    "#         input_tensor = torch.cat((input_tensor[:, 1:, :], next_position_tensor), dim=1)\n",
    "    \n",
    "#     next_position_np = next_position.numpy()\n",
    "#     next_position_inverse = scaler.inverse_transform(next_position_np)\n",
    "\n",
    "#     return next_position_inverse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ais_test_data_path = '../../Project materials/ais_test.csv'\n",
    "ais_data_test = pd.read_csv(ais_test_data_path)\n",
    "unique_ship_ids = ais_data_test['vesselId'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for idx, row in ais_data_test.iterrows():\n",
    "#     ship_id_sample = row['vesselId']\n",
    "#     prediction_time = row['time']\n",
    "\n",
    "#     # Predict the position at the specified future time\n",
    "#     predicted_position = predict_ship_position(ship_id=ship_id_sample, time=prediction_time, model=loaded_GRU_model)\n",
    "\n",
    "#     if predicted_position is not None:\n",
    "#         #print(f\"Predicted position for ship_id {ship_id} at {prediction_time}: {predicted_position}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    ship_id                time  predicted_latitude  \\\n",
      "0  61e9f3aeb937134a3c4bfe3d 2024-05-08 00:03:00           31.631407   \n",
      "1  61e9f473b937134a3c4c02df 2024-05-08 00:06:00           14.870845   \n",
      "2  61e9f469b937134a3c4c029b 2024-05-08 00:10:00           38.603489   \n",
      "3  61e9f45bb937134a3c4c0221 2024-05-08 00:11:00          -42.677868   \n",
      "4  61e9f38eb937134a3c4bfd8d 2024-05-08 00:12:00           48.747246   \n",
      "\n",
      "   predicted_longitude  predicted_sog  predicted_cog  \\\n",
      "0           -83.850716       0.074459     194.464005   \n",
      "1           120.107689       0.042288      29.438742   \n",
      "2            10.837335      18.094183      32.100861   \n",
      "3           172.166824       0.359098     189.451187   \n",
      "4            -6.218265       1.373619     228.019608   \n",
      "\n",
      "   predicted_movement_status  \n",
      "0                  -0.975770  \n",
      "1                  -0.988630  \n",
      "2                   0.994855  \n",
      "3                  -0.928127  \n",
      "4                   0.319609  \n"
     ]
    }
   ],
   "source": [
    "##Attempt at a faster approach:\n",
    "\n",
    "\n",
    "ship_predictions = defaultdict(dict)\n",
    "\n",
    "for ship_id in unique_ship_ids:\n",
    "    if ship_id not in ship_train_dataframes:\n",
    "        print(f\"No training data available for ship_id: {ship_id}\")\n",
    "        continue\n",
    "\n",
    "    df = ship_train_dataframes[ship_id]\n",
    "    \n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    \n",
    "    df['hour'] = df['time'].dt.hour\n",
    "    df['minute'] = df['time'].dt.minute\n",
    "    df['second'] = df['time'].dt.second\n",
    "\n",
    "    features = df[['longitude', 'latitude', 'sog', 'cog', 'movement_status']].values\n",
    "    if len(features) < sequence_length:\n",
    "        print(f\"Not enough historical data to predict for ship_id: {ship_id}\")\n",
    "        continue\n",
    "\n",
    "    input_sequence = features[-sequence_length:]\n",
    "    scaler = MinMaxScaler().fit(features)\n",
    "    input_sequence_normalized = scaler.transform(input_sequence)\n",
    "\n",
    "    input_tensor = torch.tensor(input_sequence_normalized, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    ship_test_times = pd.to_datetime(ais_data_test[ais_data_test['vesselId'] == ship_id]['time'])\n",
    "    farthest_time = ship_test_times.max()\n",
    "\n",
    "    last_known_time = df['time'].iloc[-1]\n",
    "    total_steps_needed = int((farthest_time - last_known_time).total_seconds() // (20 * 60))\n",
    "\n",
    "    current_input = input_tensor.clone()\n",
    "\n",
    "    for step in range(1, total_steps_needed + 1):\n",
    "        with torch.no_grad():\n",
    "            next_position = loaded_GRU_model(current_input)\n",
    "\n",
    "        prediction_time = last_known_time + pd.Timedelta(minutes=20 * step)\n",
    "        prediction_np = next_position.cpu().numpy()\n",
    "        prediction_original_scale = scaler.inverse_transform(prediction_np.reshape(1, -1))\n",
    "        \n",
    "        ship_predictions[ship_id][prediction_time] = prediction_original_scale[0]\n",
    "\n",
    "        current_input = torch.cat((current_input[:, 1:, :], next_position.unsqueeze(0)), dim=1)\n",
    "\n",
    "predictions_list = []\n",
    "\n",
    "for idx, row in ais_data_test.iterrows():\n",
    "    ship_id = row['vesselId']\n",
    "    target_time = pd.to_datetime(row['time']).round('min')\n",
    "\n",
    "    target_time_str = target_time.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "    if ship_id in ship_predictions:\n",
    "        if target_time_str in ship_predictions[ship_id]:\n",
    "            prediction = ship_predictions[ship_id][target_time_str]\n",
    "        else:\n",
    "\n",
    "            available_times = list(ship_predictions[ship_id].keys())\n",
    "            closest_time_str = min(available_times, key=lambda x: abs(pd.Timestamp(x) - target_time))\n",
    "            prediction = ship_predictions[ship_id][closest_time_str]\n",
    "\n",
    "        predictions_list.append({\n",
    "            'ship_id': ship_id,\n",
    "            'time': target_time,\n",
    "            'predicted_latitude': prediction[1],  \n",
    "            'predicted_longitude': prediction[0],\n",
    "            'predicted_sog': prediction[2],\n",
    "            'predicted_cog': prediction[3],\n",
    "            'predicted_movement_status': prediction[4]\n",
    "        })\n",
    "\n",
    "predictions_df = pd.DataFrame(predictions_list)\n",
    "\n",
    "print(predictions_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitude: Min = -213.85166931152344, Max = 177.6414031982422\n",
      "Latitude: Min = -59.9287223815918, Max = 60.745361328125\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check the minimum and maximum values for longitude\n",
    "longitude_min = predictions_df['predicted_longitude'].min()\n",
    "longitude_max = predictions_df['predicted_longitude'].max()\n",
    "\n",
    "# Check the minimum and maximum values for latitude\n",
    "latitude_min = predictions_df['predicted_latitude'].min()\n",
    "latitude_max = predictions_df['predicted_latitude'].max()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Longitude: Min = {longitude_min}, Max = {longitude_max}\")\n",
    "print(f\"Latitude: Min = {latitude_min}, Max = {latitude_max}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = predictions_df[['predicted_longitude', 'predicted_latitude']]\n",
    "\n",
    "# Write to CSV file with a header\n",
    "csv_file_path = 'submission.csv'\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "csv_data.to_csv(csv_file_path, index=True, index_label = 'ID', header=['longitude_predicted','latitude_predicted'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 First attempt to improve model accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.51 Inspecting ship-specific data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vessel-specific data:\n",
    "- Ship-type\n",
    "- Draft\n",
    "- Length\n",
    "- Breadth\n",
    "- Depth\n",
    "- Engine Power\n",
    "- Top speed\n",
    "- longitude and latitude of destination port\n",
    "- longditude and latitude of home port\n",
    "- longditude and latutude of ais-port\n",
    "\n",
    "Time-series data:\n",
    "- Time since sailing\n",
    "- Time untill arrival\n",
    "- Mooring status (True if sog is close to zero, longitude and latitude does not change for a certain time, if the longitude and latitude are close to the known port-destinations)\n",
    "- AIS data (position, cog, sog, rot, heading, ETARAW, navstat)\n",
    "\n",
    "\n",
    "Ideas to derived features:\n",
    "- Current speed percentage of max speed\n",
    "- Historical average speed over different time frames\n",
    "- Heading relative to destination port - suggests if the ship is taking a detour or not\n",
    "- Changes in heading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.52 Preprocessing relevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ais_train_data_path = '../../Project materials/ais_train.csv'\n",
    "ports_data_path = '../../Project materials/ports.csv'\n",
    "vessels_data_path = '../../Project materials/vessels.csv'\n",
    "\n",
    "\n",
    "ais_data_train = pd.read_csv(ais_train_data_path, sep='|')\n",
    "ports = pd.read_csv(ports_data_path, sep='|')\n",
    "vessels = pd.read_csv(vessels_data_path, sep='|')\n",
    "\n",
    "ship_train_ais_groups = ais_data_train.groupby('vesselId')\n",
    "ship_train_dataframes = {ship_id: group for ship_id, group in ship_train_ais_groups}\n",
    "\n",
    "\n",
    "vessels = vessels.set_index('vesselId')\n",
    "#ports = ports.set_index('portId')\n",
    "\n",
    "\n",
    "#Handle NAN values:\n",
    "columns_to_fill = ['GT', 'length', 'breadth', 'enginePower']\n",
    "\n",
    "for column in columns_to_fill:\n",
    "    median_value = vessels[column].median()  # Calculate the median value of the column\n",
    "    vessels[column] = vessels[column].fillna(median_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitude: Min = -167.54093, Max = 178.80538\n",
      "Latitude: Min = -47.53287, Max = 70.5572\n"
     ]
    }
   ],
   "source": [
    "longitude_min = 0\n",
    "longitude_max = 0\n",
    "\n",
    "latitude_min = 0\n",
    "latitude_max = 0\n",
    "\n",
    "for ship_id, df in ship_train_dataframes.items():\n",
    "\n",
    "\n",
    "# Check the minimum and maximum values for longitude\n",
    "    longitude_ship_min = df['longitude'].min()\n",
    "    longitude_ship_max = df['longitude'].max()\n",
    "\n",
    "# Check the minimum and maximum values for latitude\n",
    "    latitude_ship_min = df['latitude'].min()\n",
    "    latitude_ship_max = df['latitude'].max()\n",
    "\n",
    "    if longitude_max < longitude_ship_max:\n",
    "        longitude_max = longitude_ship_max\n",
    "    if longitude_min > longitude_ship_min:\n",
    "        longitude_min = longitude_ship_min\n",
    "    if latitude_max < latitude_ship_max:\n",
    "        latitude_max = latitude_ship_max\n",
    "    if latitude_min > latitude_ship_min:\n",
    "        latitude_min = latitude_ship_min\n",
    "    \n",
    "\n",
    "# Print the results\n",
    "print(f\"Longitude: Min = {longitude_min}, Max = {longitude_max}\")\n",
    "print(f\"Latitude: Min = {latitude_min}, Max = {latitude_max}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values in each column:\n",
      "shippingLineId      0\n",
      "CEU                 0\n",
      "DWT                 8\n",
      "GT                  0\n",
      "NT                524\n",
      "vesselType         12\n",
      "breadth             0\n",
      "depth             469\n",
      "draft             701\n",
      "enginePower         0\n",
      "freshWater        490\n",
      "fuel              490\n",
      "homePort          138\n",
      "length              0\n",
      "maxHeight         676\n",
      "maxSpeed          498\n",
      "maxWidth          676\n",
      "rampCapacity      677\n",
      "yearBuilt           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'df' is your DataFrame\n",
    "nan_summary = vessels.isna().sum()\n",
    "\n",
    "# Print out the summary of NaN values for each column\n",
    "print(\"Number of NaN values in each column:\")\n",
    "print(nan_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define scalers\n",
    "time_series_scaler = MinMaxScaler()\n",
    "vessel_features_scaler = MinMaxScaler()\n",
    "\n",
    "sequence_length = 5\n",
    "\n",
    "# Collect all vessel features for scaling\n",
    "all_vessel_features = []\n",
    "all_features = []\n",
    "\n",
    "for ship_id, df in ship_train_dataframes.items():\n",
    "    if ship_id in vessels.index:\n",
    "        vessel_features = vessels.loc[ship_id][['length', 'breadth', 'enginePower', 'GT']].values\n",
    "        all_vessel_features.append(vessel_features)\n",
    "\n",
    "    # Collect time-series features for scaling\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    #df['etaRaw'] = df['etaRaw'].apply(lambda x: f\"2024-{x}\" if pd.notna(x) and isinstance(x, str) else np.nan)\n",
    "    #df['etaRaw'] = pd.to_datetime(df['etaRaw'], format='%Y-%m-%d %H:%M', errors='coerce')\n",
    "    df['hour'] = df['time'].dt.hour\n",
    "    df['minute'] = df['time'].dt.minute\n",
    "    df['second'] = df['time'].dt.second\n",
    "    #df['time_to_eta'] = (df['etaRaw'] - df['time']).dt.total_seconds() / 3600\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        ais_features = [\n",
    "            row['hour'], row['minute'], row['second'], \n",
    "            row['longitude'], row['latitude'], row['sog'], \n",
    "            row['cog'], row['rot'], row['heading'], row['navstat']\n",
    "        ]\n",
    "        all_features.append(ais_features)\n",
    "\n",
    "# Convert collected features to numpy arrays and fit the scalers\n",
    "all_vessel_features = np.array(all_vessel_features, dtype=np.float32)\n",
    "all_features = np.array(all_features, dtype=np.float32)\n",
    "\n",
    "vessel_features_scaler.fit(all_vessel_features)\n",
    "time_series_scaler.fit(all_features)\n",
    "\n",
    "# Transform each ship's data consistently using the fitted scalers\n",
    "all_timeseries = []\n",
    "vessel_features_list = []\n",
    "\n",
    "for ship_id, df in ship_train_dataframes.items():\n",
    "    if ship_id in vessels.index:\n",
    "        # Extract and scale vessel-specific features\n",
    "        vessel_features = vessels.loc[ship_id][['length', 'breadth', 'enginePower', 'GT']].values.reshape(1, -1)\n",
    "        vessel_features = vessel_features_scaler.transform(vessel_features).flatten()\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    # Extract and transform time-series features using the same fitted scaler\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    # df['etaRaw'] = df['etaRaw'].apply(lambda x: f\"2024-{x}\" if pd.notna(x) and isinstance(x, str) else np.nan)\n",
    "    # df['etaRaw'] = pd.to_datetime(df['etaRaw'], format='%Y-%m-%d %H:%M', errors='coerce')\n",
    "    df['hour'] = df['time'].dt.hour\n",
    "    df['minute'] = df['time'].dt.minute\n",
    "    df['second'] = df['time'].dt.second\n",
    "    #df['time_to_eta'] = (df['etaRaw'] - df['time']).dt.total_seconds() / 3600\n",
    "\n",
    "    features_list = []\n",
    "    for idx, row in df.iterrows():\n",
    "        ais_features = [\n",
    "            row['hour'], row['minute'], row['second'], \n",
    "            row['longitude'], row['latitude'], row['sog'], \n",
    "            row['cog'], row['rot'], row['heading'], row['navstat']\n",
    "        ]\n",
    "        features_list.append(ais_features)\n",
    "\n",
    "    # Convert to numpy array and transform using the scaler\n",
    "    features = np.array(features_list, dtype=np.float32)\n",
    "    features_normalized = time_series_scaler.transform(features)\n",
    "\n",
    "    # Create time-series data for training\n",
    "    for i in range(len(features_normalized) - sequence_length):\n",
    "        timeseries = features_normalized[i:i + sequence_length + 1]\n",
    "        all_timeseries.append(timeseries)\n",
    "\n",
    "        # Store vessel-specific features for each sequence\n",
    "        vessel_features_list.append(vessel_features)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "all_timeseries = np.array(all_timeseries, dtype=np.float32)\n",
    "vessel_features_list = np.array(vessel_features_list, dtype=np.float32)\n",
    "\n",
    "# Split time-series into X (input sequences) and Y (target values)\n",
    "X_data = all_timeseries[:, :-1, :]  # Shape: (num_samples, sequence_length, num_features)\n",
    "Y_data = all_timeseries[:, -1, :]  # Shape: (num_samples, num_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "nan_summary = np.isnan(all_features).sum(axis=0)\n",
    "\n",
    "\n",
    "print(nan_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1518629, 6, 10)\n",
      "(1518629, 5, 10)\n",
      "(1518629, 10)\n",
      "(1518629, 4)\n"
     ]
    }
   ],
   "source": [
    "print(all_timeseries.shape)\n",
    "print(X_data.shape)\n",
    "print(Y_data.shape)\n",
    "print(vessel_features_list.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.53 Adapting the GRU model to handle non-timestep features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUnetExtended(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, non_timestep_features_size):\n",
    "        super(GRUnetExtended, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_size + non_timestep_features_size, output_size)\n",
    "\n",
    "    def forward(self, X, non_timestep_features):\n",
    "\n",
    "        hidden_initialize = torch.zeros(self.num_layers, X.size(0), self.hidden_size).to(X.device)\n",
    "\n",
    "        out, _ = self.gru(X, hidden_initialize)\n",
    "\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        combined = torch.cat((out, non_timestep_features), dim=1)\n",
    "\n",
    "        out = self.fc(combined)\n",
    "\n",
    "        out = torch.tanh(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#Model parameters:\n",
    "input_size = 10\n",
    "hidden_size = 64   #Random guess on what is best\n",
    "output_size = 10     \n",
    "num_layers = 2 \n",
    "non_timestep_features_size = 4\n",
    "\n",
    "extended_GRU_model = GRUnetExtended(input_size=input_size, hidden_size=hidden_size, output_size=output_size, num_layers=num_layers, non_timestep_features_size=non_timestep_features_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.54 Training the extended model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShipDataset(Dataset):\n",
    "    def __init__(self, X_data, vessel_features_list, Y_data):\n",
    "        self.X_data = torch.tensor(X_data, dtype=torch.float32)\n",
    "        self.vessel_features = torch.tensor(vessel_features_list, dtype=torch.float32)\n",
    "        self.Y_data = torch.tensor(Y_data, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'time_series': self.X_data[idx],\n",
    "            'vessel_features': self.vessel_features[idx],\n",
    "            'target': self.Y_data[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0191\n",
      "Epoch [2/10], Loss: 0.0163\n",
      "Epoch [3/10], Loss: 0.0155\n",
      "Epoch [4/10], Loss: 0.0146\n",
      "Epoch [5/10], Loss: 0.0144\n",
      "Epoch [6/10], Loss: 0.0143\n",
      "Epoch [7/10], Loss: 0.0142\n",
      "Epoch [8/10], Loss: 0.0141\n",
      "Epoch [9/10], Loss: 0.0140\n",
      "Epoch [10/10], Loss: 0.0140\n"
     ]
    }
   ],
   "source": [
    "all_timeseries = np.array(all_timeseries, dtype=np.float32)\n",
    "vessel_features_list = np.array(vessel_features_list, dtype=np.float32)\n",
    "Y_data = np.array(Y_data, dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "dataset = ShipDataset(X_data, vessel_features_list, Y_data)\n",
    "train_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(extended_GRU_model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    extended_GRU_model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        # Extract the features from the batch\n",
    "        time_series = batch['time_series']\n",
    "        vessel_features = batch['vessel_features']\n",
    "        target = batch['target']\n",
    "\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = extended_GRU_model(time_series, vessel_features)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "# Print the gradient norms for all parameters\n",
    "        for name, param in extended_GRU_model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                grad_norm = param.grad.norm()\n",
    "        \n",
    "                if grad_norm > 10:  # Threshold for detecting unusually large gradients\n",
    "                    print(f\"Warning: Unusually large gradient detected for {name}: {grad_norm}\")\n",
    "\n",
    "\n",
    "        #torch.nn.utils.clip_grad_norm_(extended_GRU_model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track the loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print the average loss for each epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.save(extended_GRU_model.state_dict(), \"extended_gru_model_test.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRUnetExtended(\n",
       "  (gru): GRU(10, 64, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=68, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_extended_GRU_model = GRUnetExtended(input_size=input_size, hidden_size=hidden_size, output_size=output_size, num_layers=num_layers, non_timestep_features_size=non_timestep_features_size)\n",
    "loaded_extended_GRU_model.load_state_dict(torch.load(\"extended_gru_model_test.pth\"))\n",
    "\n",
    "loaded_extended_GRU_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "ais_test_data_path = '../../Project materials/ais_test.csv'\n",
    "ais_data_test = pd.read_csv(ais_test_data_path)\n",
    "unique_ship_ids = ais_data_test['vesselId'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dictionary to store predictions for each ship\n",
    "ship_predictions = defaultdict(dict)\n",
    "\n",
    "for ship_id in unique_ship_ids:\n",
    "    if ship_id not in ship_train_dataframes:\n",
    "        print(f\"No training data available for ship_id: {ship_id}\")\n",
    "        continue\n",
    "\n",
    "    df = ship_train_dataframes[ship_id]\n",
    "\n",
    "    # Convert time column to datetime\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "    # Extract time features\n",
    "    df['hour'] = df['time'].dt.hour\n",
    "    df['minute'] = df['time'].dt.minute\n",
    "    df['second'] = df['time'].dt.second\n",
    "\n",
    "    # Calculate `time_to_eta`\n",
    "    # df['etaRaw'] = df['etaRaw'].apply(lambda x: f\"2024-{x}\" if pd.notna(x) else x)\n",
    "    # df['etaRaw'] = pd.to_datetime(df['etaRaw'], format='%Y-%m-%d %H:%M', errors='coerce')\n",
    "    # df['time_to_eta'] = (df['etaRaw'] - df['time']).dt.total_seconds() / 3600  # Time to ETA in hours\n",
    "\n",
    "    # # Extract schedule-specific features (e.g., port latitude and longitude)\n",
    "    # df['port_lat'] = df['portId'].apply(lambda x: ports.loc[x]['latitude'] if pd.notna(x) and x in ports.index else np.nan)\n",
    "    # df['port_lon'] = df['portId'].apply(lambda x: ports.loc[x]['longitude'] if pd.notna(x) and x in ports.index else np.nan)\n",
    "\n",
    "    # Extract features used during training\n",
    "    features = df[['hour', 'minute', 'second', 'longitude', 'latitude', 'sog', 'cog', 'rot', 'heading', 'navstat']]\n",
    "\n",
    "    # Handle missing values with different strategies based on feature context using .loc[]\n",
    "    # features.loc[:, 'longitude'] = features['longitude'].fillna(features['longitude'].mean())\n",
    "    # features.loc[:, 'latitude'] = features['latitude'].fillna(features['latitude'].mean())\n",
    "    # features.loc[:, 'sog'] = features['sog'].fillna(features['sog'].median())\n",
    "    # features.loc[:, 'cog'] = features['cog'].fillna(features['cog'].median())\n",
    "    # features.loc[:, 'time_to_eta'] = features['time_to_eta'].fillna(features['time_to_eta'].median())\n",
    "    # features.loc[:, ['port_lat', 'port_lon']] = features[['port_lat', 'port_lon']].fillna(features[['port_lat', 'port_lon']].median())\n",
    "\n",
    "    # # Convert features to numpy array after handling NaN values\n",
    "    features = features.values\n",
    "\n",
    "    if len(features) < sequence_length:\n",
    "        print(f\"Not enough historical data to predict for ship_id: {ship_id}\")\n",
    "        continue\n",
    "\n",
    "    # Use the already fitted time-series scaler to transform features\n",
    "    input_sequence_normalized = time_series_scaler.transform(features[-sequence_length:])\n",
    "    input_tensor = torch.tensor(input_sequence_normalized, dtype=torch.float32).unsqueeze(0)  # Shape: (1, sequence_length, num_features)\n",
    "\n",
    "    # Extract and normalize vessel-specific features using the already fitted scaler\n",
    "    if ship_id in vessels.index:\n",
    "        vessel_features = vessels.loc[ship_id][['length', 'breadth', 'enginePower', 'GT']].values.reshape(1, -1)\n",
    "        vessel_features_normalized = vessel_features_scaler.transform(vessel_features).flatten()\n",
    "    else:\n",
    "        print(f\"No vessel-specific features available for ship_id: {ship_id}\")\n",
    "        continue\n",
    "\n",
    "    vessel_features_tensor = torch.tensor(vessel_features_normalized, dtype=torch.float32).unsqueeze(0)  # Shape: (1, vessel_feature_size)\n",
    "\n",
    "    # Get the farthest prediction time needed from test data\n",
    "    ship_test_times = pd.to_datetime(ais_data_test[ais_data_test['vesselId'] == ship_id]['time'])\n",
    "    farthest_time = ship_test_times.max()\n",
    "\n",
    "    # Last known time in the training data\n",
    "    last_known_time = df['time'].iloc[-1]\n",
    "    total_steps_needed = int((farthest_time - last_known_time).total_seconds() // (20 * 60))  # Time difference in steps of 20 minutes\n",
    "\n",
    "    # Make predictions recursively for the required number of steps\n",
    "    current_input = input_tensor.clone()\n",
    "\n",
    "    for step in range(1, total_steps_needed + 1):\n",
    "        with torch.no_grad():\n",
    "            # Make the prediction using the model\n",
    "            next_position = loaded_extended_GRU_model(current_input, vessel_features_tensor)\n",
    "\n",
    "        # Prediction time for the next step\n",
    "        prediction_time = last_known_time + pd.Timedelta(minutes=20 * step)\n",
    "\n",
    "        # Convert the prediction back to the original scale for storing\n",
    "        prediction_np = next_position.cpu().numpy()\n",
    "        prediction_original_scale = time_series_scaler.inverse_transform(prediction_np.reshape(1, -1))\n",
    "\n",
    "        # Store the prediction\n",
    "        ship_predictions[ship_id][prediction_time] = prediction_original_scale[0]\n",
    "\n",
    "        # Re-normalize the prediction to feed it back into the model\n",
    "        prediction_normalized = time_series_scaler.transform(prediction_original_scale)\n",
    "\n",
    "        # Update the input tensor by removing the oldest time step and appending the new normalized prediction\n",
    "        next_position_tensor = torch.tensor(prediction_normalized, dtype=torch.float32).unsqueeze(0)\n",
    "        current_input = torch.cat((current_input[:, 1:, :], next_position_tensor), dim=1)\n",
    "\n",
    "\n",
    "# Create a list to store the final predictions for the test data\n",
    "predictions_list = []\n",
    "\n",
    "for idx, row in ais_data_test.iterrows():\n",
    "    ship_id = row['vesselId']\n",
    "    target_time = pd.to_datetime(row['time']).round('min')\n",
    "\n",
    "    if ship_id in ship_predictions:\n",
    "        if target_time in ship_predictions[ship_id]:\n",
    "            prediction = ship_predictions[ship_id][target_time]\n",
    "        else:\n",
    "            # If the exact target time is not found, find the closest prediction time\n",
    "            available_times = list(ship_predictions[ship_id].keys())\n",
    "            closest_time = min(available_times, key=lambda x: abs(pd.Timestamp(x) - target_time))\n",
    "            prediction = ship_predictions[ship_id][closest_time]\n",
    "\n",
    "        predictions_list.append({\n",
    "            'ship_id': ship_id,\n",
    "            'time': target_time,\n",
    "            'predicted_latitude': prediction[4],  # Assuming latitude is at index 4\n",
    "            'predicted_longitude': prediction[3],  # Assuming longitude is at index 3\n",
    "            'predicted_sog': prediction[5],  # Assuming speed over ground (sog) is at index 5\n",
    "            'predicted_cog': prediction[6]  # Assuming course over ground (cog) is at index 6\n",
    "        })\n",
    "\n",
    "# Convert the predictions list to a DataFrame\n",
    "predictions_df = pd.DataFrame(predictions_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitude: Min = -122.76395416259766, Max = 166.85350036621094\n",
      "Latitude: Min = -72.84368133544922, Max = 61.318416595458984\n"
     ]
    }
   ],
   "source": [
    "# Check the minimum and maximum values for longitude\n",
    "longitude_min = predictions_df['predicted_longitude'].min()\n",
    "longitude_max = predictions_df['predicted_longitude'].max()\n",
    "\n",
    "# Check the minimum and maximum values for latitude\n",
    "latitude_min = predictions_df['predicted_latitude'].min()\n",
    "latitude_max = predictions_df['predicted_latitude'].max()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Longitude: Min = {longitude_min}, Max = {longitude_max}\")\n",
    "print(f\"Latitude: Min = {latitude_min}, Max = {latitude_max}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = predictions_df[['predicted_longitude', 'predicted_latitude']]\n",
    "\n",
    "# Write to CSV file with a header\n",
    "csv_file_path = 'submission2.csv'\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "csv_data.to_csv(csv_file_path, index=True, index_label = 'ID', header=['longitude_predicted','latitude_predicted'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
