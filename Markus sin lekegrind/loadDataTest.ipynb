{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Thoughts and ideas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.1 General"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One possible strategy:\n",
    "- Treat the prediction of future AIS data as a prediction task itself (X: Historic AIS and positions, Y: AIS data in next timestep) and create a model for this\n",
    "- Use the predicted AIS data as well as historic AIS data and positions to predict new position. \n",
    "\n",
    "\n",
    "Another:\n",
    "- Let a model use the previous timesteps to predict all information about the next timestep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.2 About the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.3 Research\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Article evaluating several models to predict ship trajectories\n",
    "\n",
    "Definitions:\n",
    "- Ship trajectory is the sequence if timestamped points Pi = {Ti, LATi, LONi, SOGi, COGi}\n",
    "\n",
    "\n",
    "Methodology:\n",
    "- Information from the first four timestamps are used to predict the next.\n",
    "- Implemented using a Pytorch framework\n",
    "- Use ADAM as optimizer\n",
    "- Use the following hyperparameters: Learning rate: 0.0001, epoch: 100, dropout: 0.5, Hidden size:128 (15), input/output dimensions: 2 and hidden layer: 1\n",
    "\n",
    "\n",
    "Interesting points\n",
    "- \"Deep learning exhibits remarkable performance in AIS data-driven ship trajectory prediction\"\n",
    "- \"Deep learning are in general better than machine learning for this application\"\n",
    "- Transformer, BI-GRU and GRU performs the best, transformer only outperforms on medium sized datasets\n",
    "- SVR is the best machine learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Brainstorming - 18.09.2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AIS - data:\n",
    "- Parameters that intuitively give us the next position (COG, SOG and ROT), (current position, ETARAW and PortID)\n",
    "- Should try merging navstat codes used to describe the same activity\n",
    "\n",
    "\n",
    "\n",
    "General:\n",
    "- Should somehow allow the algorithm to keep the last values - research different strategies (CNN or LSTM?)\n",
    "- Might want to use a classifier to predict features\n",
    "- Ship-ID is probably a pointless input for the classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Other\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gustav & co brukte autogluon: https://auto.gluon.ai/stable/index.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Load data into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>cog</th>\n",
       "      <th>sog</th>\n",
       "      <th>rot</th>\n",
       "      <th>heading</th>\n",
       "      <th>navstat</th>\n",
       "      <th>etaRaw</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>vesselId</th>\n",
       "      <th>portId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01 00:00:25</td>\n",
       "      <td>284.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>01-09 23:00</td>\n",
       "      <td>-34.74370</td>\n",
       "      <td>-57.85130</td>\n",
       "      <td>61e9f3a8b937134a3c4bfdf7</td>\n",
       "      <td>61d371c43aeaecc07011a37f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-01 00:00:36</td>\n",
       "      <td>109.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6</td>\n",
       "      <td>347</td>\n",
       "      <td>1</td>\n",
       "      <td>12-29 20:00</td>\n",
       "      <td>8.89440</td>\n",
       "      <td>-79.47939</td>\n",
       "      <td>61e9f3d4b937134a3c4bff1f</td>\n",
       "      <td>634c4de270937fc01c3a7689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-01 00:01:45</td>\n",
       "      <td>111.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>01-02 09:00</td>\n",
       "      <td>39.19065</td>\n",
       "      <td>-76.47567</td>\n",
       "      <td>61e9f436b937134a3c4c0131</td>\n",
       "      <td>61d3847bb7b7526e1adf3d19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-01 00:03:11</td>\n",
       "      <td>96.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "      <td>1</td>\n",
       "      <td>12-31 20:00</td>\n",
       "      <td>-34.41189</td>\n",
       "      <td>151.02067</td>\n",
       "      <td>61e9f3b4b937134a3c4bfe77</td>\n",
       "      <td>61d36f770a1807568ff9a126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-01 00:03:51</td>\n",
       "      <td>214.0</td>\n",
       "      <td>19.7</td>\n",
       "      <td>0</td>\n",
       "      <td>215</td>\n",
       "      <td>0</td>\n",
       "      <td>01-25 12:00</td>\n",
       "      <td>35.88379</td>\n",
       "      <td>-5.91636</td>\n",
       "      <td>61e9f41bb937134a3c4c0087</td>\n",
       "      <td>634c4de270937fc01c3a74f3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time    cog   sog  rot  heading  navstat       etaRaw  \\\n",
       "0  2024-01-01 00:00:25  284.0   0.7    0       88        0  01-09 23:00   \n",
       "1  2024-01-01 00:00:36  109.6   0.0   -6      347        1  12-29 20:00   \n",
       "2  2024-01-01 00:01:45  111.0  11.0    0      112        0  01-02 09:00   \n",
       "3  2024-01-01 00:03:11   96.4   0.0    0      142        1  12-31 20:00   \n",
       "4  2024-01-01 00:03:51  214.0  19.7    0      215        0  01-25 12:00   \n",
       "\n",
       "   latitude  longitude                  vesselId                    portId  \n",
       "0 -34.74370  -57.85130  61e9f3a8b937134a3c4bfdf7  61d371c43aeaecc07011a37f  \n",
       "1   8.89440  -79.47939  61e9f3d4b937134a3c4bff1f  634c4de270937fc01c3a7689  \n",
       "2  39.19065  -76.47567  61e9f436b937134a3c4c0131  61d3847bb7b7526e1adf3d19  \n",
       "3 -34.41189  151.02067  61e9f3b4b937134a3c4bfe77  61d36f770a1807568ff9a126  \n",
       "4  35.88379   -5.91636  61e9f41bb937134a3c4c0087  634c4de270937fc01c3a74f3  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "ais_train_data_path = '../../Project materials/ais_train.csv'\n",
    "\n",
    "\n",
    "ais_data_train = pd.read_csv(ais_train_data_path, sep='|')\n",
    "\n",
    "\n",
    "ais_data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframes for each ship-id:\n",
    "\n",
    "ship_train_groups = ais_data_train.groupby('vesselId')\n",
    "ship_train_dataframes = {ship_id: group for ship_id, group in ship_train_groups}\n",
    "\n",
    "#Split data into input and output. Input can now be accessed as ship_dataframes[shipID][0] and output as ship_dataframes[shipID][1]\n",
    "\n",
    "# for key in ship_train_dataframes:\n",
    "#     ship_train_dataframes[key] = [ship_train_dataframes[key].drop(columns=['latitude', 'longitude']), ship_train_dataframes[key][['latitude', 'longitude']]]\n",
    "\n",
    "\n",
    "# print(ship_train_dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Split data into X and Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Try to create predictions using simple models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 XG-boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_simple = xgb.XGBClassifier()\n",
    "\n",
    "\n",
    "# for key in ship_train_dataframes:\n",
    "#     xgb_simple.fit(ship_train_dataframes[key][0], ship_train_dataframes[key][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Attempting to implement a similar approach as in the article: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Preprocess data into timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1518629, 6, 7)\n",
      "(1518629, 5, 7)\n",
      "(1518629, 7)\n"
     ]
    }
   ],
   "source": [
    "all_timeseries = []\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "sequence_length = 5\n",
    "\n",
    "\n",
    "for ship_id, df in ship_train_dataframes.items():\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "    df['hour'] = df['time'].dt.hour\n",
    "    df['minute'] = df['time'].dt.minute\n",
    "    df['second'] = df['time'].dt.second\n",
    "\n",
    "    features = df[['hour', 'minute', 'second', 'longitude', 'latitude', 'sog', 'cog']].values\n",
    "\n",
    "    features_normalized = scaler.fit_transform(features)\n",
    "\n",
    "    for i in range(len(features_normalized) - sequence_length):\n",
    "        timeseries = features_normalized[i:i+sequence_length+1]\n",
    "        all_timeseries.append(timeseries)\n",
    "\n",
    "\n",
    "all_timeseries = np.array(all_timeseries)\n",
    "\n",
    "X_data = all_timeseries[:, :-1, :]\n",
    "Y_data = all_timeseries[:, -1, :]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(all_timeseries.shape)\n",
    "print(X_data.shape)\n",
    "print(Y_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 GRU - model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUnet(\n",
      "  (gru): GRU(7, 64, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=64, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class GRUnet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(GRUnet, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        hidden_initialize = torch.zeros(self.num_layers, X.size(0), self.hidden_size).to(X.device)\n",
    "\n",
    "        out, _ = self.gru(X, hidden_initialize)\n",
    "\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "#Model parameters:\n",
    "input_size = 7\n",
    "hidden_size = 64   #Random guess on what is best\n",
    "output_size = 7     \n",
    "num_layers = 2 \n",
    "\n",
    "GRU_model = GRUnet(input_size=input_size, hidden_size=hidden_size, output_size=output_size, num_layers=num_layers)\n",
    "\n",
    "print(GRU_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0161\n",
      "Epoch [2/10], Loss: 0.0156\n",
      "Epoch [3/10], Loss: 0.0136\n",
      "Epoch [4/10], Loss: 0.0193\n",
      "Epoch [5/10], Loss: 0.0098\n",
      "Epoch [6/10], Loss: 0.0167\n",
      "Epoch [7/10], Loss: 0.0118\n",
      "Epoch [8/10], Loss: 0.0149\n",
      "Epoch [9/10], Loss: 0.0184\n",
      "Epoch [10/10], Loss: 0.0135\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters \n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(GRU_model.parameters(), lr = learning_rate)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "\n",
    "#Preprocess data:\n",
    "\n",
    "X_tensor = torch.tensor(X_data, dtype=torch.float32)\n",
    "Y_tensor = torch.tensor(Y_data, dtype=torch.float32)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_tensor, Y_tensor)\n",
    "data_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle = True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in data_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = GRU_model(inputs)\n",
    "\n",
    "        loss = loss_function(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Save model\n",
    "\n",
    "torch.save(GRU_model.state_dict(), \"gru_model_test.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRUnet(\n",
       "  (gru): GRU(7, 64, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=64, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_GRU_model = GRUnet(input_size=input_size, hidden_size=hidden_size, output_size=output_size, num_layers=num_layers)\n",
    "loaded_GRU_model.load_state_dict(torch.load(\"gru_model_test.pth\"))\n",
    "\n",
    "loaded_GRU_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def predict_ship_position(ship_id, time, model, sequence_length = 5):\n",
    "\n",
    "#     if ship_id not in ship_train_dataframes:\n",
    "#         print(f\"No training data available for ship_id: {ship_id}\")\n",
    "#         return None\n",
    "    \n",
    "#     ship_df = ship_train_dataframes[ship_id]\n",
    "\n",
    "#     ship_df['time'] = pd.to_datetime(ship_df['time'])\n",
    "\n",
    "#     ship_df = ship_df.sort_values(by='time').reset_index(drop=True)\n",
    "\n",
    "#     prediction_time = pd.to_datetime(time)\n",
    "\n",
    "#     last_known_time = df['time'].iloc[-1]\n",
    "#     time_diff = (prediction_time - last_known_time).total_seconds()\n",
    "    \n",
    "#     if time_diff <= 0:\n",
    "#         print(f\"Prediction time {prediction_time} is before or equal to the last known time {last_known_time}\")\n",
    "#         return None\n",
    "    \n",
    "#     steps_needed = int(time_diff//20)\n",
    "\n",
    "#     df['hour'] = df['time'].dt.hour\n",
    "#     df['minute'] = df['time'].dt.minute\n",
    "#     df['second'] = df['time'].dt.second\n",
    "\n",
    "#     features = df[['hour', 'minute', 'second', 'longitude', 'latitude', 'sog', 'cog']].values\n",
    "\n",
    "#     if len(features) < sequence_length:\n",
    "#         print(f\"Not enough historical data to predict for ship_id: {ship_id}\")\n",
    "#         return None\n",
    "\n",
    "#     input_sequence = features[-sequence_length:]\n",
    "\n",
    "#     input_sequence_normalized =scaler.transform(input_sequence)\n",
    "#     input_tensor = torch.tensor(input_sequence_normalized, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "#     for _ in range(steps_needed):\n",
    "#         with torch.no_grad():\n",
    "           \n",
    "#             next_position = model(input_tensor)\n",
    "\n",
    "        \n",
    "#         next_position_tensor = next_position.unsqueeze(0)\n",
    "#         input_tensor = torch.cat((input_tensor[:, 1:, :], next_position_tensor), dim=1)\n",
    "    \n",
    "#     next_position_np = next_position.numpy()\n",
    "#     next_position_inverse = scaler.inverse_transform(next_position_np)\n",
    "\n",
    "#     return next_position_inverse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ais_test_data_path = '../../Project materials/ais_test.csv'\n",
    "ais_data_test = pd.read_csv(ais_test_data_path)\n",
    "unique_ship_ids = ais_data_test['vesselId'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for idx, row in ais_data_test.iterrows():\n",
    "#     ship_id_sample = row['vesselId']\n",
    "#     prediction_time = row['time']\n",
    "\n",
    "#     # Predict the position at the specified future time\n",
    "#     predicted_position = predict_ship_position(ship_id=ship_id_sample, time=prediction_time, model=loaded_GRU_model)\n",
    "\n",
    "#     if predicted_position is not None:\n",
    "#         #print(f\"Predicted position for ship_id {ship_id} at {prediction_time}: {predicted_position}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    ship_id                time  predicted_latitude  \\\n",
      "0  61e9f3aeb937134a3c4bfe3d 2024-05-08 00:03:00           31.613583   \n",
      "1  61e9f473b937134a3c4c02df 2024-05-08 00:06:00           14.981131   \n",
      "2  61e9f469b937134a3c4c029b 2024-05-08 00:10:00           38.514565   \n",
      "3  61e9f45bb937134a3c4c0221 2024-05-08 00:11:00          -42.319794   \n",
      "4  61e9f38eb937134a3c4bfd8d 2024-05-08 00:12:00           48.373402   \n",
      "\n",
      "   predicted_longitude  predicted_sog  predicted_cog  \n",
      "0           -87.975906       0.676197     195.341293  \n",
      "1           120.511551       0.643126      24.003500  \n",
      "2            10.919418      18.040033      88.664772  \n",
      "3           171.528503       0.095447     200.180939  \n",
      "4            -6.027080       1.329763     238.518478  \n"
     ]
    }
   ],
   "source": [
    "##Attempt at a faster approach:\n",
    "\n",
    "\n",
    "\n",
    "# Dictionary to store predictions for each ship\n",
    "\n",
    "ship_predictions = defaultdict(dict)\n",
    "\n",
    "# Predict for each unique ship_id and store results\n",
    "for ship_id in unique_ship_ids:\n",
    "    if ship_id not in ship_train_dataframes:\n",
    "        print(f\"No training data available for ship_id: {ship_id}\")\n",
    "        continue\n",
    "\n",
    "    df = ship_train_dataframes[ship_id]\n",
    "    \n",
    "    # Ensure 'time' is in datetime format\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    \n",
    "    # Extract the last known sequence for this ship\n",
    "    df['hour'] = df['time'].dt.hour\n",
    "    df['minute'] = df['time'].dt.minute\n",
    "    df['second'] = df['time'].dt.second\n",
    "\n",
    "    features = df[['hour', 'minute', 'second', 'longitude', 'latitude', 'sog', 'cog']].values\n",
    "    if len(features) < sequence_length:\n",
    "        print(f\"Not enough historical data to predict for ship_id: {ship_id}\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the input sequence\n",
    "    input_sequence = features[-sequence_length:]\n",
    "    scaler = MinMaxScaler().fit(features)\n",
    "    input_sequence_normalized = scaler.transform(input_sequence)\n",
    "\n",
    "    # Convert to tensor and move to device\n",
    "    input_tensor = torch.tensor(input_sequence_normalized, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    # Determine the farthest time in the test data for this ship\n",
    "    ship_test_times = pd.to_datetime(ais_data_test[ais_data_test['vesselId'] == ship_id]['time'])\n",
    "    farthest_time = ship_test_times.max()\n",
    "\n",
    "    # Calculate how many steps are needed to reach the farthest time\n",
    "    last_known_time = df['time'].iloc[-1]\n",
    "    # Each step is 20 minutes, so we divide the total seconds by (20 * 60)\n",
    "    total_steps_needed = int((farthest_time - last_known_time).total_seconds() // (20 * 60))\n",
    "\n",
    "    # Predict all future steps up to the farthest time\n",
    "    current_input = input_tensor.clone()\n",
    "\n",
    "    for step in range(1, total_steps_needed + 1):\n",
    "        with torch.no_grad():\n",
    "            # Predict the next step\n",
    "            next_position = loaded_GRU_model(current_input)\n",
    "\n",
    "        # Store the prediction with its corresponding timestamp\n",
    "        prediction_time = last_known_time + pd.Timedelta(minutes=20 * step)\n",
    "        prediction_np = next_position.cpu().numpy()\n",
    "        prediction_original_scale = scaler.inverse_transform(prediction_np.reshape(1, -1))\n",
    "        \n",
    "        # Add the prediction to the dictionary for the ship ID and timestamp\n",
    "        ship_predictions[ship_id][prediction_time] = prediction_original_scale[0]\n",
    "\n",
    "        # Update the input tensor by removing the oldest step and adding the predicted next step\n",
    "        current_input = torch.cat((current_input[:, 1:, :], next_position.unsqueeze(0)), dim=1)\n",
    "\n",
    "# Create a DataFrame to store the final results for each test point\n",
    "predictions_list = []\n",
    "\n",
    "# Iterate over the test data and extract the prediction from the stored dictionary\n",
    "for idx, row in ais_data_test.iterrows():\n",
    "    ship_id = row['vesselId']\n",
    "    target_time = pd.to_datetime(row['time']).round('min')\n",
    "\n",
    "    # Convert target time to string for lookup\n",
    "    target_time_str = target_time.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "    # Retrieve the stored prediction for this ship at the target time\n",
    "    if ship_id in ship_predictions:\n",
    "        if target_time_str in ship_predictions[ship_id]:\n",
    "            # Exact match found\n",
    "            prediction = ship_predictions[ship_id][target_time_str]\n",
    "        else:\n",
    "            # No exact match, find the closest timestamp\n",
    "            available_times = list(ship_predictions[ship_id].keys())\n",
    "            closest_time_str = min(available_times, key=lambda x: abs(pd.Timestamp(x) - target_time))\n",
    "            prediction = ship_predictions[ship_id][closest_time_str]\n",
    "\n",
    "        predictions_list.append({\n",
    "            'ship_id': ship_id,\n",
    "            'time': target_time,\n",
    "            'predicted_latitude': prediction[4],  # assuming columns are ['hour', 'minute', 'second', 'longitude', 'latitude', 'sog', 'cog']\n",
    "            'predicted_longitude': prediction[3],\n",
    "            'predicted_sog': prediction[5],\n",
    "            'predicted_cog': prediction[6]\n",
    "        })\n",
    "\n",
    "# Convert predictions to a DataFrame\n",
    "predictions_df = pd.DataFrame(predictions_list)\n",
    "\n",
    "print(predictions_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310434\n"
     ]
    }
   ],
   "source": [
    "print(predictions_df.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = predictions_df[['predicted_longitude', 'predicted_latitude']]\n",
    "\n",
    "# Write to CSV file with a header\n",
    "csv_file_path = 'submission.csv'\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "csv_data.to_csv(csv_file_path, index=True, index_label = 'ID', header=['longitude_predicted','latitude_predicted'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
